{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Large Datasets:PLACES LOCAL Data for Better Health.\n",
    "\n",
    "The purpose of this project is to accomplish two main tasks. \n",
    "1) Test Whether or not States with more PLACES locations have different means that those with fewer locations, and how they differ exactly.\n",
    "2) Sucessfully work with very large data sets: Cleaning, organizinng and performing analyisis on them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "from distfit import distfit\n",
    "import statsmodels.api as sm \n",
    "import pylab\n",
    "from numpy import cov\n",
    "from itertools import permutations \n",
    "import itertools\n",
    "from scipy.stats import spearmanr\n",
    "from random import sample \n",
    "import random \n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os  \n",
    "from scipy.stats import iqr\n",
    "import scikit_posthocs as sp\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Background, Loading, Inspection\n",
    "\n",
    "\n",
    "There are many ways to acess the files that are needed. Either downloading from github, \\\n",
    "or if the files are too big  we can use the shortcuts themselves. And download right from the source. \\\n",
    "The files can be found below. The next few cells load in the data and put it into the current working directory. \n",
    "\n",
    "\n",
    "\n",
    "https://catalog.data.gov/dataset/places-local-data-for-better-health-place-data-2022-release/resource/4bfea8ab-534b-4f2b-9cb1-d0b951709c2a \n",
    "\n",
    "https://catalog.data.gov/dataset/places-local-data-for-better-health-place-data-2021-release-06a9b/resource/f9bb4b0d-7db2-432e-bf3e-58bd400a6ffc \n",
    "\n",
    "https://catalog.data.gov/dataset/places-local-data-for-better-health-place-data-2020-release-670b7/resource/b95272f6-c03d-487b-81fa-460d4b9bcb1f \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "print('Downloading started')\n",
    "url = 'https://github.com/oohtmeel1/Data_Mining_Final/raw/main/s2020.zip'\n",
    "\n",
    "# Downloading the file by sending the request to the URL\n",
    "req = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if req.status_code == 200:\n",
    "    # Create a BytesIO object from the content\n",
    "    zip_content = io.BytesIO(req.content)\n",
    "\n",
    "    # Get the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    print(f'Current Working Directory: {current_directory}')\n",
    "\n",
    "    # Specify the directory where you want to extract the contents\n",
    "    extraction_path = 'zipped_files_now'  \n",
    "    zip_ref = zipfile.ZipFile(zip_content, 'r')\n",
    "\n",
    "    # Create the extraction directory if it doesn't exist\n",
    "    os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "    # Join the extraction path and the filename\n",
    "    extraction_file_path = os.path.join(extraction_path, 's2020.zip')\n",
    "\n",
    "    # Extract the contents\n",
    "    zip_ref.extractall(extraction_file_path)\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_path = 'zipped_files_now'\n",
    "# Specify the filename \n",
    "csv_filename = 's2020.csv'  # Replace with the actual CSV file name\n",
    "\n",
    "# Join the extraction path and the filename\n",
    "csv_file_path = os.path.join(extraction_path, csv_filename)\n",
    "\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "s2020 = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = 'https://data.cdc.gov/api/views/q8ig-wwk9/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "# Downloading the file by sending the request to the URL\n",
    "req = requests.get(url)\n",
    "\n",
    "# Check if the request was successful \n",
    "if req.status_code == 200:\n",
    "    # Create a BytesIO object from the content\n",
    "    s2021 = pd.read_csv(io.StringIO(req.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = ' https://data.cdc.gov/api/views/epbn-9bv3/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "# Downloading the file by sending the request to the URL\n",
    "req = requests.get(url)\n",
    "\n",
    "# Check if the request was successful \n",
    "if req.status_code == 200:\n",
    "    # Create a BytesIO object from the content\n",
    "    s2022 = pd.read_csv(io.StringIO(req.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2021.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2022.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Basic Data Cleaning\n",
    "All NA values are dropped. And all rows and columns have been inspected to ensure correctness.The only null values remaining are data value footnotes(Which are NaN) but are of no importance. Otherwise the Data Value Column does not containy any null values. And looking at the info shows there are no very large values.Which is a good sign that there are no random digits present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA values\n",
    "s2020['Measure'] = s2020['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2021['Measure'] = s2021['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2022['Measure'] = s2022['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2020= s2020[s2020['Data_Value'].notna()]\n",
    "s2021= s2021[s2021['Data_Value'].notna()]\n",
    "s2022= s2022[s2022['Data_Value'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(s2020['Data_Value'])) # Returns where the dataframe is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(s2021['Data_Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(s2022['Data_Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020.loc[s2020['Data_Value'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2021.loc[s2021['Data_Value'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2022.loc[s2022['Data_Value'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic visualizations\n",
    "How are the locations distributed for each state?\n",
    "The number of unique locations are counted and plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"viridis\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts2 = s2021.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts2 = city_counts2.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts2,palette=\"viridis\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts3 = s2022.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts3 = city_counts3.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts3,palette=\"viridis\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measure_counts = s2020.groupby(['StateAbbr', 'Measure']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below plot answers the obvious. Do places with more locations measured overall have a higher number of Data points obtained. Yes, the answer is yes. They also look fairly consistent year to year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts, dodge=True,legend=False)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measure_counts1 = s2021.groupby(['StateAbbr', 'Measure']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts1, dodge=True,legend=False)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State 2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measure_counts3 = s2022.groupby(['StateAbbr', 'Measure']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts3, dodge=True,legend=False)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are values divided up as far as category goes?\n",
    "How are the values divided up as far as Category  goes. There are 3. PREVENT, HLTHOUT, abd UNHBEH.\n",
    "\n",
    "HLTHOUT always have the most, PREVENT seems to be second and UNHBEH usually has the smallest number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measure_counts5 = s2020.groupby(['StateAbbr', 'CategoryID']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_counts6 = s2021.groupby(['StateAbbr', 'CategoryID']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_counts7 = s2022.groupby(['StateAbbr', 'CategoryID']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts5, hue ='CategoryID',dodge=True,legend=True)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts6, hue ='CategoryID',dodge=True,legend=True)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts7, hue ='CategoryID',dodge=True,legend=True)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to split large dataframes into smaller chunks\n",
    "It is really cumbersome trying to work with these dataframes as they are. So I am going to split them up into smaller bits.\\\n",
    "Working with such a large amounts of data can be a little bit unwieldy at first. \n",
    "So while it is not recommended for normal use, in this case \\\n",
    "I used the exec and eval functions to split the dataframes into smaller chunks\\\n",
    "Storing those in a list of dataframes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Getting measures by themselves and creating just a ton of df\n",
    "sa=s2020['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2020.groupby(s2020['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2020 = list(dataframes_dict.values())\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "sa=s2021['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2021.groupby(s2021['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2021 = list(dataframes_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting measures by themselves and creating just a ton of df\n",
    "sa=s2022['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2022.groupby(s2022['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2022 = list(dataframes_dict.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new lists of dataframes can use indexing to access where each State is. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2020[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2021[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022[0].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to deal with outliers with large amounts of data\n",
    "There are many ways to approach this problem. The biggest factor is time\\\n",
    "larger datasets require more time and care since it is easier to miss values that may not belong or outliers.\\\n",
    "While histograms and QQ plots are still useful, it might be better to use some python built-ins to help \\\n",
    "and to save time. \n",
    "The percentile functions will be used to perform this. A summary is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs the summary statistics for all unique measures in the entire dataframe by State.\n",
    "And in order to remove outliers we can use the ``` IQR ``` method. \\\n",
    "Where we set up a 'fence' outside of Q1 and Q3. Anything outside of this fence is considered an outlier.\\\n",
    "The formula for calculating the 'fence' is :\n",
    "$$\\begin{align}\n",
    "((1.5 * IQR) - Q1)  \n",
    "\\end{align}$$\n",
    "OR \n",
    "$$\\begin{align}\n",
    "((1.5 * IQR) + Q3) \n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=dataframes_list2020[50]\n",
    "len(abc.loc[abc['Measure']=='Chronic_obstructive_pulmonary_disease_among_adults_aged_>=18_years','Data_Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "#n=2\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\t#print(subseta)\n",
    "\tmatching_indices1 = [index for index, df in enumerate(dataframes_list2020) if (df['StateAbbr'].isin(subseta).any())]# No matter where they are, this could should find the matching states. \n",
    "\tfor index in matching_indices1:\n",
    "\t\tmatches_2020 = dataframes_list2020[index]\n",
    "\t\tsubset1=(matches_2020['Measure'].unique()) \n",
    "\t\tintersection_list = set(list(subset1))\n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\tlis1 = matches_2020.loc[matches_2020['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t#print(len(lis1))\n",
    "\t\t\tmy_array = np.array(lis1)\n",
    "\t\t\tq1,q3 = np.percentile(my_array, (25,75))\n",
    "\t\t\tiqr1 = q3-q1\t\t\n",
    "\t\t\t#print(iqr)\n",
    "\t\t\tmin1 = q1-(1.5*iqr1) # Subtract from Q1\n",
    "\t\t\t#(print(min1))\n",
    "\t\t\tmax1 = (1.5*iqr1)+q3 # Add to Q3\n",
    "\t\t\tx_sum.append([subseta,min1,max1,i,q1,q3])\n",
    "\t\t\t#print(\"Total prints:\", print_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_min_max=pd.DataFrame(x_sum) # Put those in a dataframe\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max[0].astype(str)\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max['StateAbbr'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "List_of_min_max[\"StateAbbr\"] = List_of_min_max[\"StateAbbr\"].str.strip()\n",
    "List_of_min_max['min1'] = List_of_min_max[1].astype(float)\n",
    "List_of_min_max['max1'] = List_of_min_max[2].astype(float)\n",
    "List_of_min_max['Measure'] = List_of_min_max[3].astype(str)\n",
    "List_of_min_max['Measure'] = List_of_min_max['Measure'].str.strip()\n",
    "List_of_min_max['Q1'] = List_of_min_max[4].astype(float)\n",
    "List_of_min_max['Q3'] = List_of_min_max[5].astype(float)\n",
    "List_of_min_max= List_of_min_max.drop([0,1,2,3,4,5],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okee=s2020[s2020['StateAbbr']=='NC']\n",
    "abc2=pd.DataFrame(okee.loc[okee['Measure']=='Older_adult_women_aged_>=65_years_who_are_up_to_date_on_a_core_set_of_clinical_preventive_services:_Flu_shot_past_year,_PPV_shot_ever,_Colorectal_cancer_screening,_and_Mammogram_past_2_years','Data_Value'])\n",
    "abc2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentdf=dataframes_list2020[0] # Manually Checking My data\n",
    "experimentdf1=dataframes_list2020[1] \n",
    "experimentdf2=dataframes_list2020[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentdf1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=s2020.columns.values.tolist()\n",
    "dataframes_list2020_1 =[] #<- New Dataframe list\n",
    "n = len(dataframes_list2020)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "for j in range(n):\n",
    "\n",
    "\tmatches_2020 = dataframes_list2020[j] # <- Change this matches 2020 is first dataframe\n",
    "\tmatches_2020['Measure'] = matches_2020['Measure'].str.strip()\n",
    "\tmatches_2020['StateAbbr']=matches_2020['StateAbbr'].str.strip()\n",
    "\tmatches_2020['Data_Value']=matches_2020['Data_Value'].astype(int)\n",
    "\t#print(matches_2020['StateAbbr'].unique())\n",
    "\txbar = len(matches_2020['Measure'].unique()) # match all of the unique values\n",
    "\tlong_dataframe_list=[]\n",
    "\tfor k in range(xbar):\n",
    "\t\n",
    "\t\tfirst_row = List_of_min_max.iloc[k] # Matches the same index in my new df because they should be the same\n",
    "\t\t#print(first_row)\n",
    "\t\tstate_abbr = first_row[0]\n",
    "\t\t#print(state_abbr)\n",
    "\t\tmeasure = first_row[3]\n",
    "\t\t#print(measure)\n",
    "\t\tmaxes1 = first_row[2].astype(int)\n",
    "\t\t#print(maxes1)\n",
    "\t\tmins1 = first_row[1].astype(int)\n",
    "\t\t#print(mins1)\n",
    "\t\tcondition = matches_2020[matches_2020['Measure']== measure]\n",
    "\t\tfiltering_now = condition[condition['Data_Value'] >= mins1]\n",
    "\t\tfiltering_now1 = filtering_now[filtering_now['Data_Value'] <= maxes1]\n",
    "\t\t#result_df = pd.concat([result_df,filtering_now1])\n",
    "\t\tresult_df1= filtering_now1.values.tolist()\n",
    "\t\tlong_dataframe_list.extend(result_df1)\n",
    "\t\t\n",
    "\tresults_test=pd.DataFrame(long_dataframe_list,columns=column_names)\n",
    "\tdataframes_list2020_1.append(results_test) # Remember to change this after debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2020_1[1].loc[dataframes_list2020_1[1]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of outliers for 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum1=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2021)\n",
    "\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\t#print(subseta)\n",
    "\tmatching_indices1 = [index for index, df in enumerate(dataframes_list2021) if (df['StateAbbr'].isin(subseta).any())]# No matter where they are, this could should find the matching states. \n",
    "\tfor index in matching_indices1:\n",
    "\t\tmatches_2021 = dataframes_list2021[index]\n",
    "\t\tsubset1=(matches_2021['Measure'].unique()) \n",
    "\t\tintersection_list = set(list(subset1))\n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\tlis1 = matches_2021.loc[matches_2021['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t#print(len(lis1))\n",
    "\t\t\tmy_array = np.array(lis1)\n",
    "\t\t\tq1,q3 = np.percentile(my_array, (25,75))\n",
    "\t\t\tiqr1 = q3-q1\t\t\n",
    "\t\t\t#print(iqr)\n",
    "\t\t\tmin1 = q1-(1.5*iqr1) # Subtract from Q1\n",
    "\t\t\t#(print(min1))\n",
    "\t\t\tmax1 = (1.5*iqr1)+q3 # Add to Q3\n",
    "\t\t\tx_sum1.append([subseta,min1,max1,i,q1,q3])\n",
    "\t\t\t#print(\"Total prints:\", print_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_min_max=pd.DataFrame(x_sum1) # Put those in a dataframe\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max[0].astype(str)\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max['StateAbbr'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "List_of_min_max[\"StateAbbr\"] = List_of_min_max[\"StateAbbr\"].str.strip()\n",
    "List_of_min_max['min1'] = List_of_min_max[1].astype(float)\n",
    "List_of_min_max['max1'] = List_of_min_max[2].astype(float)\n",
    "List_of_min_max['Measure'] = List_of_min_max[3].astype(str)\n",
    "List_of_min_max['Measure'] = List_of_min_max['Measure'].str.strip()\n",
    "List_of_min_max['Q1'] = List_of_min_max[4].astype(float)\n",
    "List_of_min_max['Q3'] = List_of_min_max[5].astype(float)\n",
    "List_of_min_max= List_of_min_max.drop([0,1,2,3,4,5],axis=1)\n",
    "List_of_min_max_2021 =List_of_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=s2021.columns.values.tolist()\n",
    "dataframes_list2021_1 =[] #<- New Dataframe list\n",
    "n = len(dataframes_list2021)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "for j in range(n):\n",
    "\n",
    "\tmatches_2021 = dataframes_list2021[j] # <- Change this matches 2020 is first dataframe\n",
    "\tmatches_2021['Measure'] = matches_2021['Measure'].str.strip()\n",
    "\tmatches_2021['StateAbbr']=matches_2021['StateAbbr'].str.strip()\n",
    "\tmatches_2021['Data_Value']=matches_2021['Data_Value'].astype(int)\n",
    "\t#print(matches_2020['StateAbbr'].unique())\n",
    "\txbar = len(matches_2021['Measure'].unique()) # match all of the unique values\n",
    "\tlong_dataframe_list=[]\n",
    "\tfor k in range(xbar):\n",
    "\t\n",
    "\t\tfirst_row = List_of_min_max_2021.iloc[k] # Matches the same index in my new df because they should be the same\n",
    "\t\t#print(first_row)\n",
    "\t\tstate_abbr = first_row[0]\n",
    "\t\t#print(state_abbr)\n",
    "\t\tmeasure = first_row[3]\n",
    "\t\t#print(measure)\n",
    "\t\tmaxes1 = first_row[2].astype(int)\n",
    "\t\t#print(maxes1)\n",
    "\t\tmins1 = first_row[1].astype(int)\n",
    "\t\t#print(mins1)\n",
    "\t\tcondition = matches_2021[matches_2021['Measure']== measure]\n",
    "\t\tfiltering_now = condition[condition['Data_Value'] >= mins1]\n",
    "\t\tfiltering_now1 = filtering_now[filtering_now['Data_Value'] <= maxes1]\n",
    "\t\t#result_df = pd.concat([result_df,filtering_now1])\n",
    "\t\tresult_df1= filtering_now1.values.tolist()\n",
    "\t\tlong_dataframe_list.extend(result_df1)\n",
    "\t\t\n",
    "\tresults_test=pd.DataFrame(long_dataframe_list,columns=column_names)\n",
    "\tdataframes_list2021_1.append(results_test) # Remember to change this after debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2021_1[0]['Measure'].nunique()\n",
    "dataframes_list2021_1[0].loc[dataframes_list2021_1[0]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2021[0].loc[dataframes_list2021[0]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of outliers 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum2=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2022)\n",
    "\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\t#print(subseta)\n",
    "\tmatching_indices1 = [index for index, df in enumerate(dataframes_list2022) if (df['StateAbbr'].isin(subseta).any())]# No matter where they are, this could should find the matching states. \n",
    "\tfor index in matching_indices1:\n",
    "\t\tmatches_2022 = dataframes_list2022[index]\n",
    "\t\tsubset1=(matches_2022['Measure'].unique()) \n",
    "\t\tintersection_list = set(list(subset1))\n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\tlis1 = matches_2022.loc[matches_2022['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t#print(len(lis1))\n",
    "\t\t\tmy_array = np.array(lis1)\n",
    "\t\t\tq1,q3 = np.percentile(my_array, (25,75))\n",
    "\t\t\tiqr1 = q3-q1\t\t\n",
    "\t\t\t#print(iqr)\n",
    "\t\t\tmin1 = q1-(1.5*iqr1) # Subtract from Q1\n",
    "\t\t\t#(print(min1))\n",
    "\t\t\tmax1 = (1.5*iqr1)+q3 # Add to Q3\n",
    "\t\t\tx_sum2.append([subseta,min1,max1,i,q1,q3])\n",
    "\t\t\t#print(\"Total prints:\", print_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_min_max=pd.DataFrame(x_sum2) # Put those in a dataframe\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max[0].astype(str)\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max['StateAbbr'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "List_of_min_max[\"StateAbbr\"] = List_of_min_max[\"StateAbbr\"].str.strip()\n",
    "List_of_min_max['min1'] = List_of_min_max[1].astype(float)\n",
    "List_of_min_max['max1'] = List_of_min_max[2].astype(float)\n",
    "List_of_min_max['Measure'] = List_of_min_max[3].astype(str)\n",
    "List_of_min_max['Measure'] = List_of_min_max['Measure'].str.strip()\n",
    "List_of_min_max['Q1'] = List_of_min_max[4].astype(float)\n",
    "List_of_min_max['Q3'] = List_of_min_max[5].astype(float)\n",
    "List_of_min_max= List_of_min_max.drop([0,1,2,3,4,5],axis=1)\n",
    "List_of_min_max_2022 =List_of_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=s2022.columns.values.tolist()\n",
    "dataframes_list2022_1 =[] #<- New Dataframe list\n",
    "n = len(dataframes_list2022)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "for j in range(n):\n",
    "\n",
    "\tmatches_2022 = dataframes_list2022[j] # <- Change this matches 2020 is first dataframe\n",
    "\tmatches_2022['Measure'] = matches_2022['Measure'].str.strip()\n",
    "\tmatches_2022['StateAbbr']=matches_2022['StateAbbr'].str.strip()\n",
    "\tmatches_2022['Data_Value']=matches_2022['Data_Value'].astype(int)\n",
    "\t#print(matches_2020['StateAbbr'].unique())\n",
    "\txbar = len(matches_2022['Measure'].unique()) # match all of the unique values\n",
    "\tlong_dataframe_list=[]\n",
    "\tfor k in range(xbar):\n",
    "\t\n",
    "\t\tfirst_row = List_of_min_max_2022.iloc[k] # Matches the same index in my new df because they should be the same\n",
    "\t\t#print(first_row)\n",
    "\t\tstate_abbr = first_row[0]\n",
    "\t\t#print(state_abbr)\n",
    "\t\tmeasure = first_row[3]\n",
    "\t\t#print(measure)\n",
    "\t\tmaxes1 = first_row[2].astype(int)\n",
    "\t\t#print(maxes1)\n",
    "\t\tmins1 = first_row[1].astype(int)\n",
    "\t\t#print(mins1)\n",
    "\t\tcondition = matches_2022[matches_2022['Measure']== measure]\n",
    "\t\tfiltering_now = condition[condition['Data_Value'] >= mins1]\n",
    "\t\tfiltering_now1 = filtering_now[filtering_now['Data_Value'] <= maxes1]\n",
    "\t\t#result_df = pd.concat([result_df,filtering_now1])\n",
    "\t\tresult_df1= filtering_now1.values.tolist()\n",
    "\t\tlong_dataframe_list.extend(result_df1)\n",
    "\t\t\n",
    "\tresults_test=pd.DataFrame(long_dataframe_list,columns=column_names)\n",
    "\tdataframes_list2022_1.append(results_test) # Remember to change this after debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022_1[0]['Measure'].nunique()\n",
    "dataframes_list2022_1[0].loc[dataframes_list2022_1[0]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022[0].loc[dataframes_list2022[0]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022_1[50].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Cells seem normal. The values update accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2020 = dataframes_list2020_1\n",
    "dataframes_list2021 = dataframes_list2021_1\n",
    "dataframes_list2022 = dataframes_list2022_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay Now time for Density Plots. \n",
    "Density plots can be very useful but with huge amounts of data can be overwhelming. \n",
    "\n",
    "The below code visualizes just a few. \n",
    "In order to plot all of them would require a huge amount of computing power. So it does not make sense to do this for so much data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lmn = list(s2020['Measure'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "n = 5\n",
    "lmn = list(s2020['Measure'].unique())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "abc_index = 0  # Initialize index for lmn\n",
    "for j in range(n):\n",
    "    dataframe_needed = dataframes_list2020_1[j]\n",
    "    abc = lmn[abc_index]\n",
    "    data_time = dataframe_needed.loc[dataframe_needed['Measure'] == abc, 'Data_Value']\n",
    "    sns.kdeplot(data=data_time, fill=True, label=f'DataFrame {j + 1} - {dataframe_needed[\"StateAbbr\"].unique()}')\n",
    "\n",
    "    if j == n:\n",
    "        # Increment index for abc if it's the last dataframe\n",
    "        abc_index += 1\n",
    "\n",
    "plt.legend(title='DataFrame')\n",
    "plt.title(f'{abc}')\n",
    "plt.xlabel('Data Value')\n",
    "plt.show()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "n = 10\n",
    "lmn = list(s2020['Measure'].unique())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "abc_index = 0  # Initialize index for lmn\n",
    "for j in range(5,n):\n",
    "    dataframe_needed = dataframes_list2020_1[j]\n",
    "    abc = lmn[abc_index]\n",
    "    data_time = dataframe_needed.loc[dataframe_needed['Measure'] == abc, 'Data_Value']\n",
    "    sns.kdeplot(data=data_time, fill=True, label=f'DataFrame {j + 1} - {dataframe_needed[\"StateAbbr\"].unique()}')\n",
    "\n",
    "    if j == n:\n",
    "        # Increment index for abc if it's the last dataframe\n",
    "        abc_index += 1\n",
    "\n",
    "plt.legend(title='DataFrame')\n",
    "plt.title(f'{abc}')\n",
    "plt.xlabel('Data Value')\n",
    "plt.show()\n",
    "n = 15\n",
    "lmn = list(s2020['Measure'].unique())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "abc_index = 0  # Initialize index for lmn\n",
    "for j in range(10,n):\n",
    "    dataframe_needed = dataframes_list2020_1[j]\n",
    "    abc = lmn[abc_index]\n",
    "    data_time = dataframe_needed.loc[dataframe_needed['Measure'] == abc, 'Data_Value']\n",
    "    sns.kdeplot(data=data_time, fill=True, label=f'DataFrame {j + 1} - {dataframe_needed[\"StateAbbr\"].unique()}')\n",
    "\n",
    "    if j == n:\n",
    "        # Increment index for abc if it's the last dataframe\n",
    "        abc_index += 1\n",
    "\n",
    "plt.legend(title='DataFrame')\n",
    "plt.title(f'{abc}')\n",
    "plt.xlabel('Data Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2020_1[0][dataframes_list2020_1[0]['Measure']==\"Taking_medicine_for_high_blood_pressure_control_among_adults_aged_>=18_years_with_high_blood_pressure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020[s2020['Measure']==\"Taking_medicine_for_high_blood_pressure_control_among_adults_aged_>=18_years_with_high_blood_pressure\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what to do when visualization is unrealistic but we need to check out the distributions of the data so the right type of test can be applied?\n",
    "\n",
    "Some python functions help us out here. \n",
    "distfit is a great one for example. It can determine what the best distribution fit would be for data \\\n",
    "This was already performed elsewhere and the results are loaded in below. \n",
    "\n",
    "https://pypi.org/project/distfit/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normality_2021 = pd.read_csv('2021_data_df')\n",
    "normality_2022 = pd.read_csv('2022_data_df')\n",
    "normality_2020 = pd.read_csv('2020_data_df')\n",
    "print(('normal',len(normality_2020[normality_2020['Best_Fit_Distribution']=='norm'])),\n",
    "('not_normal',len(normality_2020[normality_2020['Best_Fit_Distribution']!='norm'])),\n",
    "('normal',len(normality_2021[normality_2021['Best_Fit_Distribution']=='norm'])),\n",
    "('not_normal',len(normality_2021[normality_2021['Best_Fit_Distribution']!='norm'])),\n",
    "('normal',len(normality_2022[normality_2022['Best_Fit_Distribution']=='norm'])),\n",
    "('not_normal',len(normality_2022[normality_2022['Best_Fit_Distribution']!='norm'])),sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the data are overwhelmingly non-normally distributed.\n",
    "\n",
    "We can use the Spearman's rank-order correlation.\n",
    "Which is the non-parametric version of the pearson product moment correlation test. \\\n",
    "It helps show the strength of the relationship between two variables. \n",
    "With a -1 or a +1 denoting perfect positive or negative relationship. \\\n",
    "So what is needed for this analysis are all of the values that are as close to 0 as possible.\n",
    "\n",
    "\n",
    "## The formula for the Spearman's rank order correlation:\n",
    "\n",
    "$$ r_s = \\rho R(X),R(Y)= \\frac{cov(R(X),R(Y))}{\\sigma_{R(X)} \\sigma_{R(Y)}} $$\n",
    "\n",
    "\n",
    "$$cov(R(X),R(Y)) = {\\text{The covariance of the rank variables}}$$ \n",
    "\n",
    "and p denotes the pearson correlation coefficient.\n",
    "\n",
    "And once all of the values are obtained, the data can be fiiltered leaving only uncorrelated pairs of variables.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_my_state(states,dataframe_list):\n",
    "\tstate = []\n",
    "\tx =[]\n",
    "\ty=[]\n",
    "\tn=len(dataframe_list)\n",
    "\tfor state in states:\n",
    "\t\tstate = set(state['StateAbbr'].unique())  # Get the unique state name\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tmatches_2020 = dataframe_list[i]\n",
    "\t\t\tsubseta=set(matches_2020['StateAbbr'].unique())\n",
    "\t\t\tif state == subseta:\n",
    "\t\t\t\tmatches_texas = dataframe_list[i]\n",
    "\t\t\t\tsubset1 = (matches_texas['Measure'].unique())\n",
    "\t\t\t\tsubsetb=(matches_texas['StateAbbr'].unique())\n",
    "\t\t\telse:\n",
    "\t\t\t\ti +=1\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tnot_texas = dataframe_list[i] # Otherwise iterate over everything\n",
    "\t\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\t\tsubset2 = (not_texas['Measure'].unique())\n",
    "\t\t\t\n",
    "\t\t\tintersection_of_measures = set(subset1).intersection(set(subset2)) # now the intersection of all 2\t\n",
    "\t\t\tintersection_list = list(intersection_of_measures)\n",
    "\n",
    "\t\t\tfor i in intersection_list:\n",
    "\t\t\t\ty.append([i])\n",
    "\t\t\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t\t\tif len(lis1) != len(lis2):\n",
    "\t\t\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif len(lis1) == min_len:\n",
    "\t\t\t\t\n",
    "\t\t\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement and also makes sure the shorter list length is used\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tx.append([subsetb,subsetc,stats.spearmanr(lis1, lis2),i])\n",
    "\treturn x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code takes a list of the dataframes, and the function. And outputs the filtered p values and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_states = [TX,CA,PA]\n",
    "yay=find_my_state(list_of_states,dataframes_list2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_states = [TX,CA,PA]\n",
    "yay1=find_my_state(list_of_states,dataframes_list2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_states = [TX,CA,PA]\n",
    "yay2=find_my_state(list_of_states,dataframes_list2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code makes the needed df for all specific variables. And cleans up. And filters. \n",
    "df1=pd.DataFrame()\n",
    "def now_make_dataframes(xm):\n",
    "\tdf1 = pd.DataFrame(xm)\n",
    "\tdf1['state'] = df1[0]+df1[1]\n",
    "\tdf1['state'] = df1['state'].astype(str)\n",
    "\tdf1['result_string'] = df1[2].astype(str) # gotta change the kurkwalis to string\n",
    "\tdf1['Corr'] = df1['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "\tdf1['p-value'] = df1['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "\tdf1['State1'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ','.join(x[0]))\n",
    "\tdf1['State2'] = df1[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ','.join(x[0]))\n",
    "\tdf1['p-value'] = df1['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "\tdf1['p-value'] = pd.to_numeric(df1['p-value'], errors='coerce') # Convert P value to numeric\n",
    "\tdf1['Measure'] = df1[3]\n",
    "\tdf1= df1.drop(['result_string',0,1,2,3],axis=1)\n",
    "\tempty_rows = df1[df1.isna().any(axis=1)]\n",
    "\tdf1 = df1[~df1.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values \n",
    "\tfiltered_df = df1[df1['Corr'] < 0.5]\n",
    "\tfiltered_df1 = filtered_df[filtered_df[\"Corr\"]>-0.5]\n",
    "\treturn filtered_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1=now_make_dataframes(yay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2=now_make_dataframes(yay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3=now_make_dataframes(yay2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking below if there are any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates10 = filtered_df1[filtered_df1.duplicated(['Measure','state'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates10 = filtered_df2[filtered_df2.duplicated(['Measure','state'])]\n",
    "duplicates10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates10 = filtered_df3[filtered_df3.duplicated(['Measure','state'])]\n",
    "duplicates10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates is a good sign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So this Value below can be used to select all of the state combinations that pass correlation test and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df1.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df2.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df3.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So given the original question: Do states with more places data have different metrics in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the counts for all of the places data divided by state, PA, TX, CA, IL, have the most by a wide margin so let's look at those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=s2020[s2020['StateAbbr']=='NC']\n",
    "m['LocationName'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "years = ['2020', '2021', '2022']\n",
    "values = [28329, 28329, 28329]\n",
    "\n",
    "\n",
    "plt.bar(years, values, color='green')\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Sum of all cities in PLACES Dataset per year')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "city_counts2 = s2021.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts2 = city_counts2.sort_values(ascending=False)\n",
    "city_counts3 = s2022.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts3 = city_counts3.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['PA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['TX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"dark:#5A9_r\")\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts2,palette=\"dark:#5A9_r\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts3,palette=\"dark:#5A9_r\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kruskal-Wallis H Test \n",
    "\n",
    "The Kruskal-Wallis H-test tests whether the null hypothesis that the population median of all of groups are equal. It is a non-parametric version of ANOVA. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs. Post hoc comparisons between groups are required to determine which groups are different.*\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html\n",
    "\n",
    "\n",
    "For the code below. I take each state index, match the same states in the other dataframes and then run all stats kruskal tests on the variables. This test can also take in arrays of different lengths so I commented out the random sampling to make them the same. \n",
    "\n",
    "I need to see if the medians of the groups are rejected more for the cities with more places data than others. Over years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kruskal(states,dataframe_list):\n",
    "\tstate = []\n",
    "\tx =[]\n",
    "\ty=[]\n",
    "\tn=len(dataframe_list)\n",
    "\tfor state in states:\n",
    "\t\tstate = set(state['StateAbbr'].unique())  # Get the unique state name\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tmatches_2020 = dataframe_list[i]\n",
    "\t\t\tsubseta=set(matches_2020['StateAbbr'].unique())\n",
    "\t\t\tif state == subseta:\n",
    "\t\t\t\tmatches_texas = dataframe_list[i]\n",
    "\t\t\t\tsubset1 = (matches_texas['Measure'].unique())\n",
    "\t\t\t\tsubsetb=(matches_texas['StateAbbr'].unique())\n",
    "\t\t\telse:\n",
    "\t\t\t\ti +=1\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tnot_texas = dataframe_list[i] # Otherwise iterate over everything\n",
    "\t\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\t\tsubset2 = (not_texas['Measure'].unique())\n",
    "\t\t\t\n",
    "\t\t\tintersection_of_measures = set(subset1).intersection(set(subset2)) # now the intersection of all 2\t\n",
    "\t\t\tintersection_list = list(intersection_of_measures)\n",
    "\n",
    "\t\t\tfor i in intersection_list:\n",
    "\t\t\t\ty.append([i])\n",
    "\t\t\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\t\t\n",
    "\t\t\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement and also makes sure the shorter list length is used\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t\t#else:\n",
    "\t\t\t\t\t\t#continue\n",
    "\n",
    "\t\t\t\tx.append([subsetb,subsetc,stats.stats.kruskal(lis1, lis2),i])\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list_1 = [TX,PA,CA]\n",
    "yay5=run_kruskal(states_list_1,dataframes_list2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list_2 = [TX,PA,CA]\n",
    "yay6=run_kruskal(states_list_2,dataframes_list2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list_3 = [TX,PA,CA]\n",
    "yay7=run_kruskal(states_list_3,dataframes_list2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yay5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code makes the needed df for all specific variables. And cleans up. And filters. \n",
    "df1=pd.DataFrame()\n",
    "def now_make_dataframes(xm):\n",
    "\tdf1 = pd.DataFrame(xm)\n",
    "\tdf1['state'] = df1[0]+df1[1]\n",
    "\tdf1['state'] = df1['state'].astype(str)\n",
    "\tdf1['result_string'] = df1[2].astype(str) # gotta change the kurkwalis to string\n",
    "\tdf1['Statistic'] = df1['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "\tdf1['p-value'] = df1['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "\tdf1['State1'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ','.join(x[0]))\n",
    "\tdf1['State2'] = df1[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ','.join(x[0]))\n",
    "\tdf1['p-value'] = df1['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "\tdf1['p-value'] = pd.to_numeric(df1['p-value'], errors='coerce') # Convert P value to numeric\n",
    "\tdf1['Measure'] = df1[3]\n",
    "\tdf1= df1.drop(['result_string',0,1,2,3],axis=1)\n",
    "\tempty_rows = df1[df1.isna().any(axis=1)]\n",
    "\tdf1 = df1[~df1.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values \n",
    "\treturn df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_kruskal_2020=now_make_dataframes(yay5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_kruskal_2021=now_make_dataframes(yay6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_kruskal_2022=now_make_dataframes(yay7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorted_df1 = filtered_df1.drop(columns=['Corr','p-value','State1','State2'],axis=1)\n",
    "shorted_df2 = filtered_df2.drop(columns=['Corr','p-value','State1','State2'],axis=1)\n",
    "shorted_df3 = filtered_df3.drop(columns=['Corr','p-value','State1','State2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorted_df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2020_kruskal = filtered_df_kruskal_2020.merge(shorted_df1, how='inner', on=['Measure','state'])\n",
    "len(result_2020_kruskal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2021_kruskal = filtered_df_kruskal_2021.merge(shorted_df2, how='inner', on=['Measure','state'])\n",
    "len(result_2021_kruskal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2022_kruskal = filtered_df_kruskal_2022.merge(shorted_df3, how='inner', on=['Measure','state'])\n",
    "len(result_2022_kruskal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/10/2023\n",
    "So to see if Texas for example differed more than any other state, I need to sum all of the times texas occurs and then sum all of the other states. The above filtered DF is where there is a difference from the median. And if texas occurs more, then.. it occurs more. It does not answer the question on if the data is higher or lower. Just that a difference exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df1['State1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df1['State2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df2['State1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(sum(Counter(filtered_df2['State2'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of the results are overwhelmingly, yes the values are different. Post Hoc analysis must be performed.The dunn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of the Dunn test \n",
    "\n",
    "Since the data are mostly nonparametric. I can only say if they are different from each other, and not whether one is higher or lower. The posthoc Dunn test performs this analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_kruskal_2020.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugh = [] # New  Empty List\n",
    "state_names = []\n",
    "\n",
    "list_of_measures1 = s2020['Measure'].unique().tolist() # Put all unique values in a list\n",
    "for i in list_of_measures1:\n",
    "\n",
    "\tresult1 = s2020[s2020['Measure'] == 'Cancer_(excluding_skin_cancer)_among_adults_aged_>=18_years'] # Go over each measure\n",
    "\tfor i in new_list_of_titles:\n",
    "\t\tcondition = lambda x: x['StateAbbr'] == i  # Gather all of the state data\n",
    "\t\tfiltered_data = result1[result1.apply(condition, axis=1)] # Filter all of the state data\n",
    "\t\tlis1 = filtered_data['Data_Value'].tolist()\n",
    "\t\tugh.append(lis1)\n",
    "\t\tstate_names.append(i)\n",
    "\tab= sp.posthoc_dunn(ugh, p_adjust = 'bonferroni')\n",
    "\tdf_dunn_result = pd.DataFrame(ab, index=ab.index, columns=ab.columns) # in order to get the data to appear appropriately, need to include index and columns\n",
    "\tfiltered_df5=df_dunn_result.where(df_dunn_result<0.05)\n",
    "\tindices = [(l, m) for l, row in enumerate(filtered_df5.values) for m, value in enumerate(row) if pd.notna(value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mapping = {index: state_names[i] for i, index in enumerate(range(len(state_names)))} # Enumerates the state names list\n",
    "state_pairs = [(state_mapping[i], state_mapping[j]) for i, j in indices] # Maps the state names listto its respective index. \n",
    "sorting_pairs = set([tuple(sorted(pair)) for pair in state_pairs]) # Okay so sort the things and make them into a set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_dunn_TX=(df_dunn_result[43].where(df_dunn_result[43]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_TX=only_dunn_TX.dropna()\n",
    "len(only_dunn_TX)\n",
    "only_dunn_CA=(df_dunn_result[4].where(df_dunn_result[4]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_CA=only_dunn_CA.dropna()\n",
    "len(only_dunn_CA)\n",
    "only_dunn_PA=(df_dunn_result[38].where(df_dunn_result[38]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_PA=only_dunn_PA.dropna()\n",
    "len(only_dunn_PA)\n",
    "only_dunn_IL=(df_dunn_result[12].where(df_dunn_result[12]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_IL=only_dunn_IL.dropna()\n",
    "len(only_dunn_IL)\n",
    "only_dunn_SD=(df_dunn_result[41].where(df_dunn_result[41]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_SD=only_dunn_SD.dropna()\n",
    "len(only_dunn_SD)\n",
    "only_dunn_WY=(df_dunn_result[50].where(df_dunn_result[50]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_WY=only_dunn_WY.dropna()\n",
    "len(only_dunn_WY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(only_dunn_TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#11/11/2023 Sorted all of the state counts\n",
    "So using the above code and visualizing it. It is easy to see that Over all Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to count rows and columns separately. \n",
    "rows, columns = zip(*indices)\n",
    "\n",
    "# Count occurrences of each row and column index\n",
    "row_counts = Counter(rows)\n",
    "column_counts = Counter(columns)\n",
    "dataframe_for_rows=pd.DataFrame.from_dict(row_counts, orient='index', columns=['Count'])\n",
    "dataframe_for_rows['States']= [state_names[row] for row in dataframe_for_rows.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows1=pd.DataFrame.from_dict(row_counts, orient='index', columns=['Count'])\n",
    "# Now I have to calculate the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows2 = dataframe_for_rows.set_index(['States'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm=dataframe_for_rows2.T\n",
    "print(hm.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows2['calculated'] = dataframe_for_rows2['Count'].apply(lambda x: 'yes' if x < 46 else 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe_for_rows2[dataframe_for_rows2['calculated']=='yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(hm,palette=\"cividis\")\n",
    "\n",
    "ax.set(xlabel=\"States\",ylabel =\"Count \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It seems a parametric set of tests will be required after all. \n",
    "But how can parametric tests be used on this data?\n",
    "\n",
    "Well. I can tell that Texas is often different from the state to which it is compared. But which direction it differs is hard to discern.So maybe it is time to employ some parametric methods after all.\n",
    "\n",
    "With large enough sample sizes (> 30 or 40), the violation of the normality assumption should not cause major problems * \\\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693611/*\n",
    "\n",
    "And each sample size is a few hundred. So all measures and States satisfy this rule. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEVENE Test\n",
    "Going to use the levene test to see if samples come from equal variance distribution\n",
    "If the resulting p-value of Levene's test is less than some significance level (typically 0.05), \\\n",
    " the obtained differences in sample variances are unlikely to have occurred based on random sampling from a \\\n",
    "population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference \\\n",
    " between the variances in the population. \n",
    "\n",
    "The scipy stats Levene test library\n",
    "\n",
    "https://en.wikipedia.org/wiki/Levene%27s_test\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.levene.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints=filtered_df1[['Measure','State1','State2']]\n",
    "n = len(prints)\n",
    "current = prints.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_levenes(states,dataframe_list):\n",
    "\tstate = []\n",
    "\tx =[]\n",
    "\ty=[]\n",
    "\tn=len(dataframe_list)\n",
    "\tfor state in states:\n",
    "\t\tstate = set(state['StateAbbr'].unique())  # Get the unique state name\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tmatches_2020 = dataframe_list[i]\n",
    "\t\t\tsubseta=set(matches_2020['StateAbbr'].unique())\n",
    "\t\t\tif state == subseta:\n",
    "\t\t\t\tmatches_texas = dataframe_list[i]\n",
    "\t\t\t\tsubset1 = (matches_texas['Measure'].unique())\n",
    "\t\t\t\tsubsetb=(matches_texas['StateAbbr'].unique())\n",
    "\t\t\telse:\n",
    "\t\t\t\ti +=1\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tnot_texas = dataframe_list[i] # Otherwise iterate over everything\n",
    "\t\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\t\tsubset2 = (not_texas['Measure'].unique())\n",
    "\t\t\t\n",
    "\t\t\tintersection_of_measures = set(subset1).intersection(set(subset2)) # now the intersection of all 2\t\n",
    "\t\t\tintersection_list = list(intersection_of_measures)\n",
    "\n",
    "\t\t\tfor i in intersection_list:\n",
    "\t\t\t\ty.append([i])\n",
    "\t\t\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\t\t\n",
    "\t\t\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement and also makes sure the shorter list length is used\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t\t#else:\n",
    "\t\t\t\t\t\t#continue\n",
    "\n",
    "\t\t\t\tx.append([subsetb,subsetc,stats.levene(lis1, lis2),i])\n",
    "\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_levene = [TX,PA,CA]\n",
    "trying_2020 = run_levenes(list_for_levene, dataframes_list2020)\n",
    "trying_2021 = run_levenes(list_for_levene, dataframes_list2021)\n",
    "trying_2022 = run_levenes(list_for_levene, dataframes_list2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame()\n",
    "def now_make_dataframes(xm):\n",
    "\tdf1 = pd.DataFrame(xm)\n",
    "\tdf1['state'] = df1[0]+df1[1]\n",
    "\tdf1['state'] = df1['state'].astype(str)\n",
    "\tdf1['result_string'] = df1[2].astype(str) # gotta change the kurkwalis to string\n",
    "\tdf1['Statistic'] = df1['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "\tdf1['p-value'] = df1['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "\tdf1['State1'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ','.join(x[0]))\n",
    "\tdf1['State2'] = df1[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ','.join(x[0]))\n",
    "\tdf1['p-value'] = df1['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "\tdf1['p-value'] = pd.to_numeric(df1['p-value'], errors='coerce') # Convert P value to numeric\n",
    "\tdf1['Measure'] = df1[3]\n",
    "\tdf1= df1.drop(['result_string',0,1,2,3],axis=1)\n",
    "\tempty_rows = df1[df1.isna().any(axis=1)]\n",
    "\tdf1 = df1[~df1.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values \n",
    "\treturn df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levene_attempt2020 = now_make_dataframes(trying_2020)\n",
    "levene_attempt2021 = now_make_dataframes(trying_2021)\n",
    "levene_attempt2022 = now_make_dataframes(trying_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levene_attempt2021.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorted_df1 = filtered_df1.drop(columns=['Corr','p-value','State1','State2'],axis=1)\n",
    "shorted_df2 = filtered_df2.drop(columns=['Corr','p-value','State1','State2'],axis=1)\n",
    "shorted_df3 = filtered_df3.drop(columns=['Corr','p-value','State1','State2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2020_levene = levene_attempt2020.merge(shorted_df1, how='right', on=['Measure','state'])\n",
    "result_2021_levene = levene_attempt2021.merge(shorted_df2, how='right', on=['Measure','state'])\n",
    "result_2022_levene = levene_attempt2022.merge(shorted_df3, how='right', on=['Measure','state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_2020_levene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_2021_levene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_2022_levene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So we have the results of the Levene test for 2020 for texas above\n",
    "Which will tell us if the values come from equal or unequal variances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T test starts Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = result_2020_levene[result_2020_levene['p-value'] < 0.05] # For levenes test lower p values mean unequal variance\n",
    "defg=result_2020_levene[result_2020_levene['p-value'] >0.05 ] #Equal variance\n",
    "prints=defg[['Measure','State1','State2']]\n",
    "n = len(prints)\n",
    "current = prints.values.tolist()\n",
    "#Levene_test_2020 =defg.groupby('state')['Measure1'].nunique()#\n",
    "#unequal_variances = df2[df2.index.isin(defg.index)]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def running_t_equal_var(levene_results,dataframe):\n",
    "\tx=[]\n",
    "\ty=[]\n",
    "\tdefg=[]\n",
    "\tprints=[]\n",
    "\tcurrent=[]\n",
    "\tdefg=levene_results[levene_results['p-value'] >0.05 ]\n",
    "\tprints=defg[['Measure','State1','State2']]\n",
    "\tn = len(prints)\n",
    "\tcurrent = prints.values.tolist()\n",
    "\tfor i in range(n):  # For i in range n, set the current list parts to separate variables.\n",
    "\t\tcurrent1 = current[i]\n",
    "\t\tmeasure=(current1[0])\n",
    "\n",
    "\t\tstate1=(current1[1]) # first second, third element\n",
    "\t\tstate2=(current1[2].strip()) # Need to strip random whitesapce\n",
    "\n",
    "\n",
    "\t\tcondition1 = dataframe.loc[dataframe['StateAbbr']==state1]\n",
    "\t\t#print(len(condition1))\n",
    "\t\tlis1 = condition1.loc[condition1['Measure']==measure,'Data_Value'] # For a dataframe .loc seems to be the simplest for performing this work\n",
    "\t\tcondition2 = dataframe.loc[dataframe['StateAbbr']==state2]\n",
    "\t\t#print(len(condition2))\n",
    "\t\tlis2 = condition2.loc[condition2['Measure']==measure,'Data_Value']\n",
    "\t\ty.append([state1,state2])\n",
    "\t\tif len(lis1) != len(lis2):\n",
    "\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\n",
    "\t\t\tif len(lis1) == min_len:\n",
    "\n",
    "\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t\n",
    "\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\t\t\t#print(len(lis1),len(lis2))\n",
    "\t\t#print(stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i)\n",
    "\t\tx.append([state1,state2,measure,stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i]) # Make sure to set the alternative to greater\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trying_t_test_2020=running_t_equal_var(result_2020_levene,s2020)\n",
    "trying_t_test_2021=running_t_equal_var(result_2021_levene,s2021)\n",
    "trying_t_test_2022=running_t_equal_var(result_2022_levene,s2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame()\n",
    "def now_make_dataframes1(xm):\n",
    "\tdf1=pd.DataFrame(xm)\n",
    "\tdf1['state'] = df1[0]+df1[1]\n",
    "\tdf1['state'] = df1['state'].astype(str)\n",
    "\tdf1['result_string'] = df1[3].astype(str) # gotta change the kurkwalis to string\n",
    "\tdf1['Statistic'] = df1['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "\tdf1['p-value'] = df1['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "\tdf1['State1'] = df1[0].astype(str)\n",
    "\tdf1['State2'] = df1[1].astype(str)\n",
    "\tdf1['p-value'] = df1['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "\tdf1['p-value'] = pd.to_numeric(df1['p-value'], errors='coerce') # Convert P value to numeric\n",
    "\tdf1['Measure'] = df1[2].astype(str)\n",
    "\tdf1['Measure'] = df1['Measure'].str.strip()\n",
    "\tdf1= df1.drop(['result_string',0,1,2,3,4],axis=1)\n",
    "\tempty_rows = df1[df1.isna().any(axis=1)]\n",
    "\tdf1 = df1[~df1.index.isin(empty_rows.index)]\n",
    "\treturn df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_equal_var_2020=now_make_dataframes1(trying_t_test_2020)\n",
    "t_test_equal_var_2021=now_make_dataframes1(trying_t_test_2021)\n",
    "t_test_equal_var_2022=now_make_dataframes1(trying_t_test_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_equal_var_2020.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T test unequal Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_t_unequal_var(levene_results,dataframe):\n",
    "\tx=[]\n",
    "\ty=[]\n",
    "\tdefg=[]\n",
    "\tprints=[]\n",
    "\tdefg=levene_results[levene_results['p-value'] <0.05 ]\n",
    "\tprints=defg[['Measure','State1','State2']]\n",
    "\tn = len(prints)\n",
    "\tcurrent = prints.values.tolist()\n",
    "\tfor i in range(n):  # For i in range n, set the current list parts to separate variables.\n",
    "\t\tcurrent1 = current[i]\n",
    "\t\tmeasure=(current1[0])\n",
    "\n",
    "\t\tstate1=(current1[1]) # first second, third element\n",
    "\t\tstate2=(current1[2].strip()) # Need to strip random whitesapce\n",
    "\n",
    "\n",
    "\t\tcondition1 = dataframe.loc[dataframe['StateAbbr']==state1]\n",
    "\t\t#print(len(condition1))\n",
    "\t\tlis1 = condition1.loc[condition1['Measure']==measure,'Data_Value'] # For a dataframe .loc seems to be the simplest for performing this work\n",
    "\t\tcondition2 = dataframe.loc[dataframe['StateAbbr']==state2]\n",
    "\t\t#print(len(condition2))\n",
    "\t\tlis2 = condition2.loc[condition2['Measure']==measure,'Data_Value']\n",
    "\t\ty.append([state1,state2])\n",
    "\t\tif len(lis1) != len(lis2):\n",
    "\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\n",
    "\t\t\tif len(lis1) == min_len:\n",
    "\n",
    "\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t\n",
    "\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\t\t\t#print(len(lis1),len(lis2))\n",
    "\t\t#print(stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i)\n",
    "\t\tx.append([state1,state2,measure,stats.ttest_ind(lis1, lis2,equal_var=False,alternative='greater'),i]) # Make sure to set the alternative to greater\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trying_t_test_2020_u=running_t_equal_var(result_2020_levene,s2020)\n",
    "trying_t_test_2021_u=running_t_equal_var(result_2021_levene,s2021)\n",
    "trying_t_test_2022_u=running_t_equal_var(result_2022_levene,s2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_2020=now_make_dataframes1(trying_t_test_2020_u)\n",
    "t_test_unequal_var_2021=now_make_dataframes1(trying_t_test_2021_u)\n",
    "t_test_unequal_var_2022=now_make_dataframes1(trying_t_test_2022_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_test_unequal_var_2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal Var visualizations For Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas=t_test_equal_var_2020[t_test_equal_var_2020['State1']=='TX']\n",
    "ca=t_test_equal_var_2020[t_test_equal_var_2020['State1']=='CA']\n",
    "pa=t_test_equal_var_2020[t_test_equal_var_2020['State1']=='PA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints=texas['State2']  # get the state all by itself\n",
    "prints=prints.unique() \n",
    "lists1=[]\n",
    "for i in prints:   \n",
    "\tabc = texas[texas['State2']==i]\n",
    "\tabcd = abc[abc['p-value'] <= 0.05]\n",
    "\tdefg = abc[abc['p-value'] > 0.05]\n",
    "\tlists1.append(['TX',len(abcd),i,len(defg)])\n",
    "\tset_of_tuples = {tuple(inner_list) for inner_list in lists1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame(set_of_tuples)\n",
    "df10['combined'] = df10[0]+'/'+df10[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = range(len(set_of_tuples))\n",
    "x_val= [x[0] for x in set_of_tuples]\n",
    "y_val = [x[1] for x in set_of_tuples]\n",
    "x_val1 = [x[2] for x in set_of_tuples]\n",
    "y_val1 =[x[3] for x in set_of_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(data=(y_val),alpha=0.9)\n",
    "ax = sns.barplot(data=y_val1, alpha=0.7)\n",
    "ax.set_xticklabels(df10['combined'], rotation='vertical', fontsize=10)\n",
    "ax.set(xlabel='States',\n",
    "       ylabel='count of p_values that are significant per state VS Texas',\n",
    "       title='Resulting P values'\n",
    "       )\n",
    "ax.text(2, 9, \"Texas = ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"blue\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(2.8, 10,\"Other States= ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"orange\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(2.8, 11,\"Texas Overlap = ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"brown\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints=ca['State2']  # get the state all by itself\n",
    "prints=prints.unique() \n",
    "lists1=[]\n",
    "for i in prints:   \n",
    "\tabc = ca[ca['State2']==i]\n",
    "\tabcd = abc[abc['p-value'] <= 0.05]\n",
    "\tdefg = abc[abc['p-value'] > 0.05]\n",
    "\tlists1.append(['CA',len(abcd),i,len(defg)])\n",
    "\tset_of_tuples = {tuple(inner_list) for inner_list in lists1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame(set_of_tuples)\n",
    "df10['combined'] = df10[0]+'/'+df10[2]\n",
    "\n",
    "X_1 = range(len(set_of_tuples))\n",
    "x_val= [x[0] for x in set_of_tuples]\n",
    "y_val = [x[1] for x in set_of_tuples]\n",
    "x_val1 = [x[2] for x in set_of_tuples]\n",
    "y_val1 =[x[3] for x in set_of_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(data=(y_val),alpha=0.9)\n",
    "ax = sns.barplot(data=y_val1, alpha=0.7)\n",
    "ax.set_xticklabels(df10['combined'], rotation='vertical', fontsize=10)\n",
    "ax.set(xlabel='States',\n",
    "       ylabel='count of p_values that are significant per state VS California',\n",
    "       title='Resulting P values'\n",
    "       )\n",
    "ax.text(2, 9, \"CA = ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"blue\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(2.8, 10,\"Other States= ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"orange\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(2.8, 11,\"CA Overlap = ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"brown\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints=pa['State2']  # get the state all by itself\n",
    "prints=prints.unique() \n",
    "lists1=[]\n",
    "for i in prints:   \n",
    "\tabc = pa[pa['State2']==i]\n",
    "\tabcd = abc[abc['p-value'] <= 0.05]\n",
    "\tdefg = abc[abc['p-value'] > 0.05]\n",
    "\tlists1.append(['PA',len(abcd),i,len(defg)])\n",
    "\tset_of_tuples = {tuple(inner_list) for inner_list in lists1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame(set_of_tuples)\n",
    "df10['combined'] = df10[0]+'/'+df10[2]\n",
    "\n",
    "X_1 = range(len(set_of_tuples))\n",
    "x_val= [x[0] for x in set_of_tuples]\n",
    "y_val = [x[1] for x in set_of_tuples]\n",
    "x_val1 = [x[2] for x in set_of_tuples]\n",
    "y_val1 =[x[3] for x in set_of_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(data=(y_val),alpha=0.9)\n",
    "ax = sns.barplot(data=y_val1, alpha=0.7)\n",
    "ax.set_xticklabels(df10['combined'], rotation='vertical', fontsize=10)\n",
    "ax.set(xlabel='States',\n",
    "       ylabel='count of p_values that are significant per state VS California',\n",
    "       title='Resulting P values'\n",
    "       )\n",
    "ax.text(2, 9, \"PA = ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"blue\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(2.8, 10,\"Other States= ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"orange\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(2.8, 11,\"PA Overlap = ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"brown\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023 USE THIS\n",
    "Simpler Is better. So just loc the result dataframe and do the lengths of each. \n",
    "The code right below takes the length of the state names list and does.. something. \\\n",
    "I guess sorts and counts and so I have to append those counts to some kind of list. \n",
    "\n",
    "But now I can visualize the data properly. By taking everything out of the set of tuples and making a bar graph like the one for the original data\n",
    "\n",
    "## TO DO 11/15/2023 The below is for unequal var\n",
    "visualize the p values for texas in front of (or behind) every other state\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints=t_test_unequal_var_2020['state2']  # get the state all by itself\n",
    "prints=prints.unique() \n",
    "lists1=[]\n",
    "for i in prints:   \n",
    "\tabc = df.loc[df['state2']==i]\n",
    "\tabcd = abc[abc['p-value'] < 0.05]\n",
    "\tdefg = abc[abc['p-value'] > 0.05]\n",
    "\tlists1.append(['TX',len(abcd),i,len(defg)])\n",
    "\tset_of_tuples = {tuple(inner_list) for inner_list in lists1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame(set_of_tuples)\n",
    "df10['combined'] = df10[0]+'/'+df10[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = range(len(set_of_tuples))\n",
    "x_val= [x[0] for x in set_of_tuples]\n",
    "y_val = [x[1] for x in set_of_tuples]\n",
    "x_val1 = [x[2] for x in set_of_tuples]\n",
    "y_val1 =[x[3] for x in set_of_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(data=(y_val),alpha=0.9)\n",
    "ax = sns.barplot(data=y_val1, alpha=0.7)\n",
    "ax.set_xticklabels(df10['combined'], rotation='vertical', fontsize=10)\n",
    "ax.set(xlabel='States',\n",
    "       ylabel='count of p_values that are significant per state VS Texas',\n",
    "       title='Resulting P values'\n",
    "       )\n",
    "ax.text(2.2, 20, \" Texas = ♦\", \n",
    "       fontsize = 12,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"blue\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(4, 19,\"All other States = ♦\", \n",
    "       fontsize = 12,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"orange\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(3.8, 18,\"Texas Overlap = ♦\", \n",
    "       fontsize = 12,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"brown\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at the dataframe above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/15/2023 \n",
    "Looking over the data, when compared about half of the values have significane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code right above is the list organization code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "city_counts2 = s2021.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts2 = city_counts2.sort_values(ascending=False)\n",
    "city_counts3 = s2022.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts3 = city_counts3.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"Greens\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above is all well and good. (11/10/2023)\n",
    "But it still does not answer my question. Just states that they are different. \n",
    "So I am going to attempt a T test on Monday. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working down here for correlations as of 11/15/2023\n",
    "\n",
    "Trying to organize my values and decide if the values should be dropped. The below code filtered out all of the combinations that had corr values that were not significant. As far as how to move forward. Not sure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
