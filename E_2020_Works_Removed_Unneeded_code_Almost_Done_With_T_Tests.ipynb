{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So given the original question: Do states with more places data have different metrics in general? (maybe experience more or less year to year change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "from distfit import distfit\n",
    "import statsmodels.api as sm \n",
    "import pylab\n",
    "from numpy import cov\n",
    "from itertools import permutations \n",
    "import itertools\n",
    "from scipy.stats import spearmanr\n",
    "from random import sample \n",
    "import random \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from collections import Counter\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First the data is read in \n",
    "The CSV files are read in using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020 =pd.read_csv(\"C:/Users/amcfa/Downloads/PLACES__Local_Data_for_Better_Health__Place_Data_2020_release.csv\")\n",
    "s2021 =pd.read_csv(\"C:/Users/amcfa/Downloads/PLACES__Local_Data_for_Better_Health__Place_Data_2021_release.csv\")\n",
    "s2022 =pd.read_csv(\"C:/Users/amcfa/Downloads/PLACES__Local_Data_for_Better_Health__Place_Data_2022_release.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lis2 = state.loc[state['Measure']==second_element,'Data_Value'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Basic Data Cleaning\n",
    "All NA values are dropped. And all rows and columns have been inspected to ensure they are digits and make sense.The only null values remaining are data value footnotes(Which are NaN) but are of no importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA values\n",
    "s2020['Measure'] = s2020['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2021['Measure'] = s2021['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2022['Measure'] = s2022['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2020= s2020[s2020['Data_Value'].notna()]\n",
    "s2021= s2021[s2021['Data_Value'].notna()]\n",
    "s2022= s2022[s2022['Data_Value'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s2020.isnull().sum(),s2021.isnull().sum(),s2022.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = s2020['Data_Value'].describe()\n",
    "b = s2021['Data_Value'].describe()\n",
    "c = s2022['Data_Value'].describe()\n",
    "print((a,b,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the correlations of Measure by measure per state. \n",
    "Step 1) Get measure and states. This will end up really big. \n",
    "The below code compiles everything by state and test. Type the name of the state (2 letters) below and a big dataframe will appear. \n",
    "I can use that to run mass correlation analysis. \n",
    "While Exec is not recommended. For this project with 153 different variables, each having 30 different possible measures, it can become extremely unwieldly very fast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Getting measures by themselves and creating just a ton of df\n",
    "sa=s2020['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2020.groupby(s2020['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2020 = list(dataframes_dict.values())\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "sa=s2021['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2021.groupby(s2021['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2021 = list(dataframes_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting measures by themselves and creating just a ton of df\n",
    "sa=s2022['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2022.groupby(s2022['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2022 = list(dataframes_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2020[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2021[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022[0].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Cells seem normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Code as of 11/08/2023 - this code works (For 2020)\n",
    "Lets Hope This code works right, and it does At least it is correct otherwise(I Checked for unique Corr values and there were almost as many as the length of the list)\n",
    "I needed correlation Values for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2020[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\tif len(lis1) != len(lis2):\n",
    "\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\tif len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\t\t#print(stats.spearmanr(lis1, lis2),i)\n",
    "\t\tx.append([subsetb,subsetc,stats.spearmanr(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\tresult1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['state'] = df1[0]+df1[1]\n",
    "df1['state'] = df1['state'].astype(str)\n",
    "df1['result_string'] = df1[2].astype(str) # gotta change the kurkwalis to string\n",
    "df1['Corr'] = df1['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df1['p-value'] = df1['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df1['State1'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df1['State2'] = df1[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df1= df1.drop(['result_string',0,1,2],axis=1)\n",
    "df1['p-value'] = df1['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df1['p-value'] = pd.to_numeric(df1['p-value'], errors='coerce') # Convert P value to numeric\n",
    "df1['Measure1'] = df1[3]\n",
    "empty_rows = df1[df1.isna().any(axis=1)]\n",
    "df1 = df1[~df1.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So this Value below can be used to select all of the state combinations that pass correlation test 11/15/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df1[df1['Corr'] < 0.5]\n",
    "filtered_df1 = filtered_df[filtered_df[\"Corr\"]>-0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df1.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(dataframes_list2021)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2021[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[]\n",
    "y=[]\n",
    "\n",
    "n = len(dataframes_list2021)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2021[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2021[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\tif len(lis1) != len(lis2):\n",
    "\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\tif len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\t\t#print(stats.spearmanr(lis1, lis2),i)\n",
    "\t\tx1.append([subsetb,subsetc,stats.spearmanr(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(x1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['state'] = df2[0]+df2[1]\n",
    "df2['state'] = df2['state'].astype(str)\n",
    "df2['result_string'] = df2[2].astype(str) # gotta change the kurkwalis to string\n",
    "df2['Corr'] = df2['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df2['p-value'] = df2['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df2['State1'] = df2[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df2['State2'] = df2[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df2= df2.drop(['result_string',0,1,2],axis=1)\n",
    "df2['p-value'] = df2['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df2['p-value'] = pd.to_numeric(df2['p-value'], errors='coerce') # Convert P value to numeric\n",
    "df2['Measure1'] = df2[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df2[df2['Corr'] < 0.5]\n",
    "filtered_df2 = filtered_df[filtered_df[\"Corr\"]>-0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df2.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(dataframes_list2022)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2022[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=[]\n",
    "y=[]\n",
    "\n",
    "n = len(dataframes_list2022)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2022[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2022[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\tif len(lis1) != len(lis2):\n",
    "\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\tif len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\t\t#print(stats.spearmanr(lis1, lis2),i)\n",
    "\t\tx2.append([subsetb,subsetc,stats.spearmanr(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(x2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['state'] = df3[0]+df3[1]\n",
    "df3['state'] = df3['state'].astype(str)\n",
    "df3['result_string'] = df3[2].astype(str) # gotta change the kurkwalis to string\n",
    "df3['Corr'] = df3['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df3['p-value'] = df3['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df3['State1'] = df3[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df3['State2'] = df3[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df3= df3.drop(['result_string',0,1,2],axis=1)\n",
    "df3['p-value'] = df3['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df3['p-value'] = pd.to_numeric(df3['p-value'], errors='coerce') # Convert P value to numeric\n",
    "df3['Measure1'] = df3[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df3[df3['Corr'] < 0.5]\n",
    "filtered_df3 = filtered_df[filtered_df[\"Corr\"]>-0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df3.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After data is collected it can now be used for visualization and such (11/08/2023) Also not needed\n",
    "\n",
    "Well I can now see if states with higher counts of PLACES data have different correlation values. Why not. First I need to check for some empty values. And there were just a few. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1 = pd.DataFrame(result1)\n",
    "df3 = pd.DataFrame(result2)\n",
    "df5 = pd.DataFrame(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I had mixed data in the column so I needed to weirdly split this thing up \n",
    "def split_column(row):  \n",
    "    return pd.Series([row[0][0], row[1]])  \n",
    "df1[['state', 'Corr']] = df1[0].apply(split_column)\n",
    "df1.drop(0, axis=1, inplace=True) # Drop the df in place. dont assign to another var\n",
    "df1[['Measure1','Measure2']] = df1[1].apply(lambda x: pd.Series(x)) # This splits the string up for us and such\n",
    "empty_cells = df1.isna() # Are there any na values\n",
    "empty_cells_count = empty_cells.sum()\n",
    "print(empty_cells_count)\n",
    "empty_rows = df1[df1.isna().any(axis=1)]\n",
    "df2 = df1[~df1.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values \n",
    "df2.drop(1,axis=1, inplace=True)\n",
    "len(df2.Corr.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I had mixed data in the column so I needed to weirdly split this thing up \n",
    "def split_column(row):  \n",
    "    return pd.Series([row[0][0], row[1]])  \n",
    "df3[['state', 'Corr']] = df3[0].apply(split_column)\n",
    "df3.drop(0, axis=1, inplace=True) # Drop the df in place. dont assign to another var\n",
    "df3[['Measure1','Measure2']] = df3[1].apply(lambda x: pd.Series(x)) # This splits the string up for us and such\n",
    "empty_cells = df3.isna() # Are there any na values\n",
    "empty_cells_count = empty_cells.sum()\n",
    "print(empty_cells_count)\n",
    "empty_rows = df3[df3.isna().any(axis=1)]\n",
    "df4 = df3[~df3.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values \n",
    "df4.drop(1,axis=1, inplace=True)\n",
    "len(df4.Corr.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I had mixed data in the column so I needed to weirdly split this thing up \n",
    "def split_column(row):  \n",
    "    return pd.Series([row[0][0], row[1]])  \n",
    "df5[['state', 'Corr']] = df5[0].apply(split_column)\n",
    "df5.drop(0, axis=1, inplace=True) # Drop the df in place. dont assign to another var\n",
    "df5[['Measure1','Measure2']] = df5[1].apply(lambda x: pd.Series(x)) # This splits the string up for us and such\n",
    "empty_cells = df5.isna() # Are there any na values\n",
    "empty_cells_count = empty_cells.sum()\n",
    "print(empty_cells_count)\n",
    "empty_rows = df5[df5.isna().any(axis=1)]\n",
    "df6 = df5[~df5.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values \n",
    "df6.drop(1,axis=1, inplace=True)\n",
    "len(df6.Corr.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/08/2023\n",
    "Maybe the first correlation Value I can look at is the overall max cor value for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a cool little form of df groupby kind of neat. Uses idxmax, which returns the max of each index\n",
    "def max_corr(group):\n",
    "    max_corr_idx = group['Corr'].idxmax()\n",
    "    return group.loc[max_corr_idx,['Corr','Measure1','Measure2']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result1 = df2.groupby('state').apply(max_corr).reset_index()  # Groupby state\n",
    "result2 = df4.groupby('state').apply(max_corr).reset_index()  # Groupby state\n",
    "result3 = df6.groupby('state').apply(max_corr).reset_index()  # Groupby state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/08/2023\n",
    "\n",
    "So viewing the results briefly, Chronic kidney disease and stroke have some correlation in this data set. And I notice the correlation values for the other two years are much much lower. It is strange. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So given the original question: Do states with more places data have higher metrics in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the counts for all of the places data divided by state, PA, TX, CA, IL, have the most by a wide margin so let's look at those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=s2020[s2020['StateAbbr']=='NC']\n",
    "m['LocationName'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "years = ['2020', '2021', '2022']\n",
    "values = [28329, 28329, 28329]\n",
    "\n",
    "\n",
    "plt.bar(years, values, color='green')\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Sum of all cities in PLACES Dataset per year')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "city_counts2 = s2021.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts2 = city_counts2.sort_values(ascending=False)\n",
    "city_counts3 = s2022.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts3 = city_counts3.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['PA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['TX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"dark:#5A9_r\")\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts2,palette=\"dark:#5A9_r\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts3,palette=\"dark:#5A9_r\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No matter what Use the cor analysis to select the rows or columns (whatever) that have little or no correlation and run statistical analysis \n",
    "Year to year there is a decrease in correlation between variables. The first year there is a wide range of correlation values, and the following years there is almost none. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And looking at mean and median values for correlations for each year and set of states, all show a similiar decrease in correlation values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations seem fairly similiar. ## Big Blob from the bottom where correlation analysis was done will go here\n",
    "\n",
    "Now time to look at actual data itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = df1[df1['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "\n",
    "city_counts1 =defg.groupby('state')['Measure1'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = df1[df1['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "\n",
    "city_counts1 =defg.groupby('state')['Measure1'].nunique()\n",
    "cleaned_df = df1[df1.index.isin(defg.index)]\n",
    "city_counts2 =cleaned_df.groupby('state')['Measure1'].nunique()\n",
    "ah1 = df1[df1['Corr'] > 0.5]\n",
    "len(ah1['Measure1'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Working down here for correlations as of 11/15/2023\n",
    "\n",
    "#Trying to organize my values and decide if the values should be dropped. The below code filtered out all of the combinations that had corr values that were not significant. As far as how to move forward. Not sure\n",
    "# I had mixed data in the column so I needed to weirdly split this thing up \n",
    "\n",
    "\n",
    "abcd = df2[df2['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "\n",
    "city_counts1 =defg.groupby('state')['Measure1'].nunique()\n",
    "cleaned_df2 = df2[df2.index.isin(defg.index)]\n",
    "city_counts2 =cleaned_df2.groupby('state')['Measure1'].nunique()\n",
    "ah2 = df2[df2['Corr'] > 0.5]\n",
    "len(ah2['Measure1'].unique())\n",
    "\n",
    "\n",
    "\n",
    "# Correlation values are good\n",
    "\n",
    "len(cleaned_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = df3[df3['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "\n",
    "city_counts3 =defg.groupby('state')['Measure1'].nunique()\n",
    "cleaned_df3 = df3[df3.index.isin(defg.index)]\n",
    "city_counts3 =cleaned_df3.groupby('state')['Measure1'].nunique()\n",
    "ah3 = df3[df3['Corr'] > 0.5]\n",
    "len(ah3['Measure1'].unique())\n",
    "len(cleaned_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''defg['Measure2'].value_counts() # Sorting in place\n",
    "\n",
    "df3=df2.sort_values(by=['Corr'])\n",
    "df3.sort_values(['Measure1', 'Measure2'], inplace=True) # Obtaining measures\n",
    "mean_values = df3['Corr'].mean\n",
    "\n",
    "df3['Combined'] = df3['Measure1'].astype(str) + '_' + df3['Measure2'].astype(str) # Combining strings\n",
    "grouped_df = df3.groupby('Combined')\n",
    "mean_values = grouped_df['Corr'].median()\n",
    "mean_values '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mean_values_reset = mean_values.reset_index()\n",
    "mean_values_reset \n",
    "filtered_series = mean_values[(mean_values < 0.5) & (mean_values > -0.5)] # Filter all of the values. \n",
    "filtered_series=filtered_series.reset_index()\n",
    "stuff_to_drop=(mean_values_reset[~mean_values_reset.Combined.isin(filtered_series.Combined)])\n",
    "print(stuff_to_drop['Combined'])\n",
    "stuff_to_keep=(df3[~df3.Combined.isin(stuff_to_drop.Combined)])\n",
    "stuff_to_keep['Combined'].nunique()\n",
    "grouped = stuff_to_keep.groupby('state') # So group each by state '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big blob for correlation analysis will stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below code tests for normality using a D’Agostino’s K^2 test. \n",
    "\n",
    "But just because the data is not normally distributed, does not mean it is Gaussian in shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "alpha = 0.05\n",
    "state= dataframes_list2020[0]\n",
    "subset1=(state['Measure'].unique())\n",
    "for i in subset1:\n",
    "\tlis1 = state.loc[state['Measure']==i,'Data_Value']\n",
    "\tstat, p = normaltest(lis1)\n",
    "\tprint('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\tif p > alpha:\n",
    " \t\tprint('Sample looks Gaussian (fail to reject H0)')\n",
    "\telse:\n",
    " \t\tprint('Sample is not Gaussian, maybe normal (reject H0)')\n",
    "\t\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originally the data was processed using the fitdist function.\n",
    "In order to define the best fit distribution of all data, to see which statistical test to employ for each state, and metric for that state. \n",
    "\n",
    "And Now I am going to use R to help me work with this data. R has great statistical packages to work with just means of data for example. Also there are few sets of data that fit into a normal distribution so I will use nonparametric tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1= pd.read_csv('2020_data_df')  # Read in the data\n",
    "df2= pd.read_csv('2021_data_df')\n",
    "df3= pd.read_csv('2022_data_df')\n",
    "trying2020 = df1[['State','Measure_Name','Best_Fit_Distribution','mean']] # Select only this subset\n",
    "trying2021 = df2[['State','Measure_Name','Best_Fit_Distribution','mean']]\n",
    "trying2022 = df3[['State','Measure_Name','Best_Fit_Distribution','mean']]\n",
    "#norm2020 = trying2020[trying2020['Best_Fit_Distribution']=='norm'] # Further Filter each DF\n",
    "#norm2021 = trying2021[trying2021['Best_Fit_Distribution']=='norm']\n",
    "#norm2022 = trying2022[trying2022['Best_Fit_Distribution']=='norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying2020 =trying2020[~trying2020['Best_Fit_Distribution'].str.contains('norm')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying2021 =trying2021[~trying2021['Best_Fit_Distribution'].str.contains('norm')]\n",
    "#trying2022 =trying2022[~trying2022['Best_Fit_Distribution'].str.contains('norm')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any normally distributed data I can test using the paired t test\n",
    "to start maybe split all of the dat on these lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying2020[['group', 'year', 'testnumber']] = trying2020['State'].apply(lambda x: pd.Series(str(x).split('_'))) # apply seems to force the data to split\n",
    "trying2021[['group', 'year', 'testnumber']] = trying2021['State'].apply(lambda x: pd.Series(str(x).split('_')))\n",
    "trying2022[['group', 'year', 'testnumber']] = trying2022['State'].apply(lambda x: pd.Series(str(x).split('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/09/2023\n",
    "Now group again by state and measure. grouped list of df,then group by measure and then compare year to year. And this is difficult because I cant just merge the DF because of their size. I would run into a memory error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is trying to go by the dataframes in place kind of. It finds the first match, then matches in the others. While it is not as fast it should hopefully not be super computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working On code down here 11/09/2023 The code below is for all Data \n",
    "\n",
    "The Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs. Post hoc comparisons between groups are required to determine which groups are different.*\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html\n",
    "\n",
    "\n",
    "For the code below. I take each state index, match the same states in the other dataframes and then run all stats kruskal tests on the variables. This test can also take in arrays of different lengths so I commented out the random sampling to make them the same. \n",
    "\n",
    "I need to see if the medians of the groups are rejected more for the cities with more places data than others. Over years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As of 11/15/2023 worked here on excluding highly correlated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below shortens the result dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above gives all results where there are no year to year changes. And it seems all states change year to year when compared to themselves. So How about just the biggest cities states tested for differences against all other states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/09/20203 The below code Compares texas to all other states for all metrics for the year 2020. \n",
    "If my lists are not the same length I need to do some repetition and take the averages no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2020[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t#else:\n",
    "\t\t\t\t#continue\n",
    "\t\t#print(stats.kruskal(lis1, lis2),i)\n",
    "\t\tx.append([subsetb,subsetc,stats.kruskal(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\tresult1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1['result_string'] = df1[0].astype(str) # gotta change the kurkwalis to string\n",
    "df1['statistic'] = df1['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df1['p-value'] = df1['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df1['State'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df1['state'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ''.join(x[0]))\n",
    "df1[['State1','State2']]=df1['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df1= df1.drop(['result_string',0],axis=1)\n",
    "df1['p-value'] = df1['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df1['p-value'] = pd.to_numeric(df1['p-value'], errors='coerce') # Convert P value to numeric\n",
    "#filtered_df = df2[df2['p-value'] < 0.05]\n",
    "df1['Measure']= df1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_2020 = df1.loc[cleaned_df.index]\n",
    "result_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 texas code stops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021 texas code starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2021)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2021 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2021[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2021)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2021 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2021[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2021[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\ty.append([i])\n",
    "\t\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\t#print(min_len)\n",
    "\t\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\t\n",
    "\t\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t\t#print(lis2)\n",
    "\t\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t#else:\n",
    "\t\t\t\t\t#continue\n",
    "\t\t\t#print(stats.kruskal(lis1, lis2),i)\n",
    "\t\t\tx.append([subsetb,subsetc,stats.kruskal(lis1, lis2),i])\n",
    "\t\t\tprint_count += 1\n",
    "\t\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t\tresult2 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.DataFrame(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['result_string'] = df3[0].astype(str) # gotta change the kurkwalis to string\n",
    "df3['statistic'] = df3['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df3['p-value'] = df3['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df3['State'] = df3[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df3['state'] = df3[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ''.join(x[0]))\n",
    "df3[['State1','State2']]=df3['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df3= df3.drop(['result_string',0],axis=1)\n",
    "df3['p-value'] = df3['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df3['p-value'] = pd.to_numeric(df3['p-value'], errors='coerce') # Convert P value to numeric\n",
    "df3['Measure']= df3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Measure'] = df3[1].astype(str)\n",
    "df3['Measure'] = df3['Measure'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "df3= df3.drop([1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df2['Measure'] = filtered_df2[3].astype(str)\n",
    "filtered_df2['Measure'] = filtered_df2['Measure'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "filtered_df2= filtered_df2.drop([3],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2['state'] = filtered_df2['state'].astype(str)\n",
    "filtered_df2['state'] = filtered_df2['state'].str.replace(\"['\", '').str.replace(\"']\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2021 = df3.merge(filtered_df2, how='inner', on=['Measure','state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of 2021 texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start  of 2022 texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2022)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2021 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2022[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2022)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2021 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2022[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2022[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\ty.append([i])\n",
    "\t\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\t#print(min_len)\n",
    "\t\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\t\n",
    "\t\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t\t#print(lis2)\n",
    "\t\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t#else:\n",
    "\t\t\t\t\t#continue\n",
    "\t\t\t#print(stats.kruskal(lis1, lis2),i)\n",
    "\t\t\tx.append([subsetb,subsetc,stats.kruskal(lis1, lis2),i])\n",
    "\t\t\tprint_count += 1\n",
    "\t\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t\tresult3 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame(result3)\n",
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['result_string'] = df4[0].astype(str) # gotta change the kurkwalis to string\n",
    "df4['statistic'] = df4['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df4['p-value'] = df4['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df4['State'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df4['state'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ''.join(x[0]))\n",
    "df4[['State1','State2']]=df4['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df4= df4.drop(['result_string',0],axis=1)\n",
    "df4['p-value'] = df4['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df4['p-value'] = pd.to_numeric(df4['p-value'], errors='coerce') # Convert P value to numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Measure'] = df4[1].astype(str)\n",
    "df4['Measure'] = df4['Measure'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "df4= df4.drop([1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3= filtered_df3.rename(columns={3:\"Measure\"})\n",
    "filtered_df3['state'] = filtered_df3['state'].astype(str)\n",
    "filtered_df3['state'] = filtered_df3['state'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "filtered_df3['Measure'] = filtered_df3['Measure'].str.strip()\n",
    "filtered_df3['state'] = filtered_df3['state'].str.strip()\n",
    "df4[\"Measure\"] = df4[\"Measure\"].str.strip()\n",
    "df4[\"state\"] = df4[\"state\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2022 = df4.merge(filtered_df3, how='inner', on=['Measure','state'])\n",
    "result_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/10/2023\n",
    "So to see if Texas for example differed more than any other state, I need to sum all of the times texas occurs and then sum all of the other states. The above filtered DF is where there is a difference from the median. And if texas occurs more, then.. it occurs more. It does not answer the question on if the data is higher or lower. Just that a difference exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df['State1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df['State1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df['State2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df1['State2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of the results are overwhelmingly, yes the values are different. Post Hoc analysis must be performed.The dunn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of the Dunn test \n",
    "\n",
    "Since the data are mostly nonparametric. I can only say if they are different from each other, and not whether one is higher or lower. But if the sample sizes are large enough normality might be able to be assumed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/10/2023\n",
    "Okay I will now Work through each DF, and gather all of the data I need. My goal is to waste as little space as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_measures1 = s2020['Measure'].unique().tolist() # Put all unique values in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_measures1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugh = [] # New  Empty List\n",
    "state_names = []\n",
    "\n",
    "list_of_measures1 = s2020['Measure'].unique().tolist() # Put all unique values in a list\n",
    "\n",
    "\n",
    "result1 = s2020[s2020['Measure'] == 'Cancer_(excluding_skin_cancer)_among_adults_aged_>=18_years'] # Go over each measure\n",
    "for i in new_list_of_titles:\n",
    "\tcondition = lambda x: x['StateAbbr'] == i  # Gather all of the state data\n",
    "\tfiltered_data = result1[result1.apply(condition, axis=1)] # Filter all of the state data\n",
    "\tlis1 = filtered_data['Data_Value'].tolist()\n",
    "\tugh.append(lis1)\n",
    "\tstate_names.append(i)\n",
    "ab= sp.posthoc_dunn(ugh, p_adjust = 'bonferroni')\n",
    "df_dunn_result = pd.DataFrame(ab, index=ab.index, columns=ab.columns) # in order to get the data to appear appropriately, need to include index and columns\n",
    "filtered_df5=df_dunn_result.where(df_dunn_result<0.05)\n",
    "indices = [(l, m) for l, row in enumerate(filtered_df5.values) for m, value in enumerate(row) if pd.notna(value)]\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mapping = {index: state_names[i] for i, index in enumerate(range(len(state_names)))} # Enumerates the state names list\n",
    "state_pairs = [(state_mapping[i], state_mapping[j]) for i, j in indices] # Maps the state names listto its respective index. \n",
    "sorting_pairs = set([tuple(sorted(pair)) for pair in state_pairs]) # Okay so sort the things and make them into a set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_dunn_TX=(df_dunn_result[43].where(df_dunn_result[43]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_TX=only_dunn_TX.dropna()\n",
    "len(only_dunn_TX)\n",
    "only_dunn_CA=(df_dunn_result[4].where(df_dunn_result[4]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_CA=only_dunn_CA.dropna()\n",
    "len(only_dunn_CA)\n",
    "only_dunn_PA=(df_dunn_result[38].where(df_dunn_result[38]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_PA=only_dunn_PA.dropna()\n",
    "len(only_dunn_PA)\n",
    "only_dunn_IL=(df_dunn_result[12].where(df_dunn_result[12]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_IL=only_dunn_IL.dropna()\n",
    "len(only_dunn_IL)\n",
    "only_dunn_SD=(df_dunn_result[41].where(df_dunn_result[41]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_SD=only_dunn_SD.dropna()\n",
    "len(only_dunn_SD)\n",
    "only_dunn_WY=(df_dunn_result[50].where(df_dunn_result[50]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_WY=only_dunn_WY.dropna()\n",
    "len(only_dunn_WY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(only_dunn_TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/11/2023 Sorted all of the state counts\n",
    "So using the above code and visualizing it. It is easy to see that Over all Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to count rows and columns separately. \n",
    "rows, columns = zip(*indices)\n",
    "\n",
    "# Count occurrences of each row and column index\n",
    "row_counts = Counter(rows)\n",
    "column_counts = Counter(columns)\n",
    "dataframe_for_rows=pd.DataFrame.from_dict(row_counts, orient='index', columns=['Count'])\n",
    "dataframe_for_rows['States']= [state_names[row] for row in dataframe_for_rows.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows1=pd.DataFrame.from_dict(row_counts, orient='index', columns=['Count'])\n",
    "# Now I have to calculate the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows2 = dataframe_for_rows.set_index(['States'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm=dataframe_for_rows2.T\n",
    "print(hm.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows2['calculated'] = dataframe_for_rows2['Count'].apply(lambda x: 'yes' if x < 46 else 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe_for_rows2[dataframe_for_rows2['calculated']=='yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(hm,palette=\"cividis\")\n",
    "\n",
    "ax.set(xlabel=\"States\",ylabel =\"Count \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well. I can tell that Texas is often different from the state to which it is compared. But which direction it differs is hard to discern.So maybe it is time to employ some parametric methods after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/11/2023\n",
    "Got all of the state names sorted and counted now need to make the graph look nicer. because it looks very cluttered at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the scatterplot function to build the bubble map\n",
    "sns.scatterplot(data=dataframe_for_rows, x=\"States\", y=\"Count\", size=\"Count\", legend=False, sizes=(20, 200),alpha=0.5)\n",
    "\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/12/2023\n",
    "\n",
    "Going to use the levene test to see if samples come from equal variance distribution\n",
    "\n",
    "If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Levene%27s_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2020[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t#else:\n",
    "\t\t\t\t#continue\n",
    "\t\t#print(stats.levene(lis1, lis2),i)\n",
    "\t\tx.append([subsetb,subsetc,stats.levene(lis1, lis2),i])\n",
    "\t\t#print_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\tresult1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df4['result_string'] = df4[0].astype(str) # gotta change the kurkwalis to string\n",
    "df4['statistic'] = df4['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df4['p-value'] = df4['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df4['State'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df4['state'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ''.join(x[0]))\n",
    "df4[['State1','State2']]=df4['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df4= df4.drop(['result_string',0],axis=1)\n",
    "df4['p-value'] = df4['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df4['p-value'] = pd.to_numeric(df4['p-value'], errors='coerce') # Convert P value to numeric\n",
    "#filtered_df = df4[df4['p-value'] < 0.05]\n",
    "#filtered_df # are there yer to year changes. This code tells us the counts\n",
    "#filtered_df1 = df4[df4['p-value'] > 0.05]\n",
    "#filtered_df1 # are there yer to year changes. This code tells us the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Measure1'] = df4[1].astype(str)\n",
    "df4['Measure1'] = df4['Measure1'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "df4[\"Measure1\"] = df4[\"Measure1\"].str.strip()\n",
    "df4[\"state\"] = df4[\"state\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1['state'] = filtered_df1['state'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "filtered_df1[\"state\"] = filtered_df1[\"state\"].str.strip()\n",
    "filtered_df1[\"Measure1\"] = filtered_df1[\"Measure1\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= df4.drop([1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df1= filtered_df1.drop([3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1= filtered_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_move = 'Measure1'\n",
    "\n",
    "# Pop the column and store it in a variable\n",
    "column_temp = df4.pop(column_to_move)\n",
    "\n",
    "# Insert the popped column at the beginning of the DataFrame\n",
    "df4.insert(0, column_to_move, column_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_move = 'state'\n",
    "\n",
    "# Pop the column and store it in a variable\n",
    "column_temp = df4.pop(column_to_move)\n",
    "\n",
    "# Insert the popped column at the beginning of the DataFrame\n",
    "df4.insert(1, column_to_move, column_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_move = 'Measure1'\n",
    "\n",
    "# Pop the column and store it in a variable\n",
    "column_temp = filtered_df_test1.pop(column_to_move)\n",
    "\n",
    "# Insert the popped column at the beginning of the DataFrame\n",
    "filtered_df_test1.insert(0, column_to_move, column_temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1=filtered_df_test1.drop(['Corr'],axis=1)\n",
    "filtered_df_test1=filtered_df_test1.drop(['p-value'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1=filtered_df_test1.drop(['State1'],axis=1)\n",
    "filtered_df_test1=filtered_df_test1.drop(['State2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df4[df4.duplicated(['Measure1','state'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df4.drop_duplicates(['Measure1','state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2020_levene = df_no_duplicates.merge(filtered_df_test1, how='right', on=['Measure1','state'])\n",
    "result_2020_levene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So we have the results of the Levene test for 2020 for texas above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T test starts Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2020[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t#else:\n",
    "\t\t\t\t#continue\n",
    "\t\t#print(stats.ttest_ind(lis1, lis2),i)\n",
    "\t\tx.append([subsetb,subsetc,stats.ttest_ind(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\tresult1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame(result1)\n",
    "df4['result_string'] = df4[0].astype(str) # gotta change the kurkwalis to string\n",
    "df4['statistic'] = df4['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df4['p-value'] = df4['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df4['State'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df4[['State1','State2']]=df4['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df4= df4.drop(['result_string',0],axis=1)\n",
    "df4['p-value'] = df4['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df4['p-value'] = pd.to_numeric(df4['p-value'], errors='coerce') # Convert P value to numeric\n",
    "#filtered_df = df4[df4['p-value'] < 0.05]\n",
    "#filtered_df # are there yer to year changes. This code tells us the counts\n",
    "#filtered_df1 = df4[df4['p-value'] > 0.05]\n",
    "#filtered_df1 # are there yer to year changes. This code tells us the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Moday work on comparing all of the T tests for everything  (11/13/2023)\n",
    "# Started on it 11/14/2023\n",
    "To do *If texas and state in lis 1 and lis 2 match AND the measure for filtered_df1. Then Run the equal variance of the scipy.stats.t test\n",
    "Otherwise run unequal variances for the T test\n",
    "\n",
    "\n",
    "* Idea Maybe count all the times all of the states were 'significant' and then put that on a graph by state \n",
    "\n",
    "\n",
    "* Filtering data by measures and then using that to run either equal variance or unequal t test\n",
    "\n",
    "* And no matter what make sure to do 'greater' because you want to compare mean 1 and mean 2. With mean1 being the state in question. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_2020_levene[\"State1\"] = result_2020_levene[\"State1\"].str.strip()\n",
    "result_2020_levene[\"State2\"] = result_2020_levene[\"State2\"].str.strip()\n",
    "result_2020_levene[\"State2\"] = result_2020_levene[\"State2\"].str.strip()\n",
    "result_2020_levene['Measure1'] = result_2020_levene[\"Measure1\"].str.strip()\n",
    "result_2020_levene['Measure'] = result_2020_levene[\"Measure1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = result_2020_levene[result_2020_levene['p-value'] < 0.05] # For levenes test lower p values mean unequal variance\n",
    "defg=result_2020_levene[result_2020_levene['p-value'] >0.05 ] #Equal variance\n",
    "\n",
    "#Levene_test_2020 =defg.groupby('state')['Measure1'].nunique()#\n",
    "#unequal_variances = df2[df2.index.isin(defg.index)]#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if we need this below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "condition1 = s2020.loc[s2020['StateAbbr']==\"TX\"]\n",
    "lis1 = condition1.loc[condition1['Measure']=='Physical_health_not_good_for_>=14_days_among_adults_aged_>=18_years','Data_Value'] # For a dataframe .loc seems to be the simplest for performing this work\n",
    "condition2 = s2020.loc[s2020['StateAbbr']=='NC']\n",
    "lis2 = condition2.loc[condition2['Measure']=='Physical_health_not_good_for_>=14_days_among_adults_aged_>=18_years','Data_Value']\n",
    "\n",
    "if len(lis1) != len(lis2):\n",
    "\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\tprint(min_len)\n",
    "\tif len(lis1) == min_len:\n",
    "\n",
    "\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\tprint(lis2)\n",
    "\tif len(lis2) == min_len:\n",
    "\t\tlis1= random.sample(list(lis1), min_len)\n",
    "print(stats.ttest_ind(lis1, lis2,equal_var=False),i)\n",
    "x.append([subsetb,subsetc,stats.ttest_ind(lis1, lis2),i])\n",
    "print_count += 1\n",
    "print(\"Total prints:\", print_count)\n",
    "result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not sure iif we need the code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below code works at least for now as of 11/14/2023\n",
    "It runs the T test on the full dataframe, matching to the reduced dataframe. And Runs the T test. Printing the T test statistic, appending everything to a list. \n",
    "\n",
    "* If the calculated p-value is below the threshold chosen for statistical significance (usually the 0.10, the 0.05, or 0.01 level), then the null hypothesis is rejected in favor of the alternative hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T test for equal variances for 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get dataframe values in a list\n",
    "prints=defg[['Measure','State1','State2']]\n",
    "n = len(prints)\n",
    "current = prints.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020['Measure'] = s2020['Measure'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(prints)\n",
    "for i in range(n):  # For i in range n, set the current list parts to separate variables.\n",
    "\tcurrent1 = current[i]\n",
    "\tmeasure=(current1[0])\n",
    "\n",
    "\tstate1=(current1[1]) # first second, third element\n",
    "\tstate2=(current1[2].strip()) # Need to strip random whitesapce\n",
    "\n",
    "\n",
    "\tcondition1 = s2020.loc[s2020['StateAbbr']==state1]\n",
    "\t#print(len(condition1))\n",
    "\tlis1 = condition1.loc[condition1['Measure']==measure,'Data_Value'] # For a dataframe .loc seems to be the simplest for performing this work\n",
    "\tcondition2 = s2020.loc[s2020['StateAbbr']==state2]\n",
    "\t#print(len(condition2))\n",
    "\tlis2 = condition2.loc[condition2['Measure']==measure,'Data_Value']\n",
    "\ty.append([state1,state2])\n",
    "\tif len(lis1) != len(lis2):\n",
    "\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\n",
    "\t\tif len(lis1) == min_len:\n",
    "\n",
    "\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\n",
    "\t\tif len(lis2) == min_len:\n",
    "\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t#print(len(lis1),len(lis2))\n",
    "\t#print(stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i)\n",
    "\tx.append([state1,state2,measure,stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i]) # Make sure to set the alternative to greater\n",
    "\tprint_count += 1\n",
    "\t#print(\"Total prints:\", print_count)\n",
    "\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['result_string'] = df[3].astype(str)\n",
    "df['t-statistic'] = df['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Extract out all of the relevant p values and t statistics\n",
    "df['p-value'] = df['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df['State1']= df[0]\n",
    "df['state2']=df[1]\n",
    "df= df.drop(['result_string',0,1,3,4],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_equal_var_p_gr = df[df['p-value'] > 0.05] # So where p value is greater\n",
    "t_test_equal_var_p_ls = df[df['p-value'] <= 0.05] # So where p value is less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/14/2023 t-test for equal variances for 2020 stops here\n",
    "*Null hypothesis is that the means are the same. And alternative is that they differ. so if p value is low, you reject the null hypothesis. Since p value is not significant, the filtered filtered_df_p_values in the above dataframe are those which there mean for each test value is not higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023\n",
    "T test for unequal variances now for 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get dataframe values in a list\n",
    "prints=abcd[['Measure','State1','State2']]\n",
    "n = len(prints)\n",
    "current = prints.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(prints)\n",
    "for i in range(n):  # For i in range n, set the current list parts to separate variables.\n",
    "\tcurrent1 = current[i]\n",
    "\tmeasure=(current1[0])\n",
    "\n",
    "\tstate1=(current1[1]) # first second, third element\n",
    "\tstate2=(current1[2].strip()) # Need to strip random whitesapce\n",
    "\n",
    "\n",
    "\tcondition1 = s2020.loc[s2020['StateAbbr']==state1]\n",
    "\t#print(len(condition1))\n",
    "\tlis1 = condition1.loc[condition1['Measure']==measure,'Data_Value'] # For a dataframe .loc seems to be the simplest for performing this work\n",
    "\tcondition2 = s2020.loc[s2020['StateAbbr']==state2]\n",
    "\t#print(len(condition2))\n",
    "\tlis2 = condition2.loc[condition2['Measure']==measure,'Data_Value']\n",
    "\ty.append([state1,state2])\n",
    "\tif len(lis1) != len(lis2):\n",
    "\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\n",
    "\t\tif len(lis1) == min_len:\n",
    "\n",
    "\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\n",
    "\t\tif len(lis2) == min_len:\n",
    "\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t#print(len(lis1),len(lis2))\n",
    "\t#print(stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i)\n",
    "\tx.append([state1,state2,measure,stats.ttest_ind(lis1, lis2,equal_var=False,alternative='greater'),i]) # Make sure to set the alternative to greater\n",
    "\tprint_count += 1\n",
    "\t#print(\"Total prints:\", print_count)\n",
    "\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(x)\n",
    "df['result_string'] = df[3].astype(str)\n",
    "df['t-statistic'] = df['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Extract out all of the relevant p values and t statistics\n",
    "df['p-value'] = df['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df['State1']= df[0]\n",
    "df['state2']=df[1]\n",
    "df= df.drop(['result_string',0,1,3,4],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_p_gr = df[df['p-value'] > 0.05] # So where p value is greater\n",
    "t_test_unequal_var_p_ls = df[df['p-value'] <= 0.05] # So where p value is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_p_gr.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_p_ls.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023 Need to Go back and fix this. \n",
    "Where means are not different. \n",
    "\n",
    "To visuaklize this maybe.. group the counts? And put it in a bar chart like the one above so the format is the similiar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"Spectral\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/14/2023\n",
    "For t-values data visualizations maybe count where p value is higher for say texas. Then count where it is not, and maybe display by state? \n",
    "\n",
    "Okay so maybe if texas p value is higher (so a low p value) add some number to texas\n",
    "Otherwise add the number to the other state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First get dataframe values in a list\n",
    "state_counts = []\n",
    "df['combined_text']=df['State1']+df['state2']\n",
    "prints=df[['p-value','State1','state2','combined_text']]\n",
    "\n",
    "n = len(prints)\n",
    "current = prints.values.tolist()\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "x=[]\n",
    "y=[]\n",
    "n = len(prints)\n",
    "for i in range(n):  # For i in range n, set the current list parts to separate variables.\n",
    "\tcurrent1 = current[i]\n",
    "\tmeasure=(current1[0])\n",
    "\n",
    "\tstate1=(current1[1]) # first second, third element\n",
    "\tstate2=(current1[2].strip()) # Need to strip random whitesapce\n",
    "\tstate_counts.append([state1,state2])\n",
    "\tset_of_tuples = {tuple(inner_list) for inner_list in state_counts} # Create a set of tuples now after all of the appending is done.\n",
    "\tlistnow=list(set_of_tuples) # list of all of my variables\n",
    "\tlist_of_dicts = [{state: 0 for state in ls} for ls in listnow] # Because I like doing things inefficiently. Back into a dictionary now.\n",
    "\n",
    "\t\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023\n",
    "The below code needs to compare what is in result list to what is in current1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that the list is there maybe iterate again?\n",
    "n = len(prints)\n",
    "current1 = prints.values.tolist()\n",
    "for i in range(n):  # For i in range n, set the current list parts to separate variables.\n",
    "\tcurrent1 = current[i]\n",
    "\tmeasure=current1[0]\n",
    "\n",
    "\tstate1=current1[1] # first second, third element\n",
    "\tstate2=current1[2].strip() # Need to strip random whitesapce\n",
    "\tcombos=current1[3]\n",
    "\tresult_list = [''.join(sublist) for sublist in listnow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023 USE THIS\n",
    "Simpler Is better. So just loc the result dataframe and do the lengths of each. \n",
    "The code right below takes the length of the state names list and does.. something. \\\n",
    "I guess sorts and counts and so I have to append those counts to some kind of list. \n",
    "\n",
    "But now I can visualize the data properly. By taking everything out of the set of tuples and making a bar graph like the one for the original data\n",
    "\n",
    "## TO DO 11/15/2023 The below is for unequal var\n",
    "visualize the p values for texas in front of (or behind) every other state\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints=df['state2']  # get the state all by itself\n",
    "prints=prints.unique() \n",
    "lists1=[]\n",
    "for i in prints:   \n",
    "\tabc = df.loc[df['state2']==i]\n",
    "\tabcd = abc[abc['p-value'] < 0.05]\n",
    "\tdefg = abc[abc['p-value'] > 0.05]\n",
    "\tlists1.append(['TX',len(abcd),i,len(defg)])\n",
    "\tset_of_tuples = {tuple(inner_list) for inner_list in lists1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame(set_of_tuples)\n",
    "df10['combined'] = df10[0]+'/'+df10[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = range(len(set_of_tuples))\n",
    "x_val= [x[0] for x in set_of_tuples]\n",
    "y_val = [x[1] for x in set_of_tuples]\n",
    "x_val1 = [x[2] for x in set_of_tuples]\n",
    "y_val1 =[x[3] for x in set_of_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(data=(y_val),alpha=0.9)\n",
    "ax = sns.barplot(data=y_val1, alpha=0.7)\n",
    "ax.set_xticklabels(df10['combined'], rotation='vertical', fontsize=10)\n",
    "ax.set(xlabel='States',\n",
    "       ylabel='count of p_values that are significant per state VS Texas',\n",
    "       title='Resulting P values'\n",
    "       )\n",
    "ax.text(2, 27, \" Texas = ♦\", \n",
    "       fontsize = 12,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"blue\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(4, 26,\"All other States = ♦\", \n",
    "       fontsize = 12,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"orange\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at the dataframe above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/15/2023 \n",
    "Looking over the data, when compared about half of the values have significane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code right above is the list organization code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "city_counts2 = s2021.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts2 = city_counts2.sort_values(ascending=False)\n",
    "city_counts3 = s2022.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts3 = city_counts3.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"Spectral\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts2 = filtered_df_p_values1.groupby('State1')[2].count()\n",
    "city_counts2 = city_counts2.sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts2,palette=\"Spectral\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above is all well and good. (11/10/2023)\n",
    "But it still does not answer my question. Just states that they are different. \n",
    "So I am going to attempt a T test on Monday. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\tprint(subseta)\n",
    "\tmatching_indices1 = [index for index, df in enumerate(dataframes_list2021) if (df['StateAbbr'].isin(subseta).any())]# No matter where they are, this could should find the matching states. \n",
    "\tfor index in matching_indices1:\n",
    "\t\tmatches_2021 = dataframes_list2021[index]\n",
    "\tmatching_indices2 = [index for index, df in enumerate(dataframes_list2022) if (df['StateAbbr'].isin(subseta).any())]\n",
    "\tfor index in matching_indices2:\n",
    "\t\tmatches_2022 = dataframes_list2022[index]\n",
    "\t\tsubset1=(matches_2020['Measure'].unique()) \n",
    "\t\tsubset2=(matches_2021['Measure'].unique())  # Now collect the unique measures from all df\n",
    "\t\tsubset3=(matches_2022['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2).intersection(subset3) # now the intersection of all 3\n",
    "\t\tintersection_list = list(intersection_of_measures)\n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\tlis1 = matches_2020.loc[matches_2020['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\tlis2 = matches_2021.loc[matches_2021['Measure']==i,'Data_Value']\n",
    "\t\t\tlis3 = matches_2022.loc[matches_2022['Measure']==i,'Data_Value'] # Now to deal with if they are not the same length\n",
    "\t\t\tif len(lis1) != len(lis2) != len(lis3):  # If the lengths of the lists are not equal I will have to deal with that\n",
    "\t\t\t\t\tmin_len = (min(len(lis1), len(lis2),len(lis3))) # Set the minimum length\n",
    "\t\t\t\t\tif len(lis1) == min_len:\n",
    "\t\t\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t\t\tlis3= random.sample(list(lis3), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t\t\tlis3= random.sample(list(lis3), min_len)\n",
    "\t\t\t\t\tif len(lis3) == min_len:\n",
    "\t\t\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t\t\tlis2= random.sample(list(lis2), min_len)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\tprint(stats.kruskal(lis1, lis2, lis3),i)\n",
    "\t\t\tx.append([subseta,stats.kruskal(lis1, lis2, lis3),i])\n",
    "\t\t\tprint_count += 1\n",
    "\t\t\tprint(\"Total prints:\", print_count)\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working down here for correlations as of 11/15/2023\n",
    "\n",
    "Trying to organize my values and decide if the values should be dropped. The below code filtered out all of the combinations that had corr values that were not significant. As far as how to move forward. Not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had mixed data in the column so I needed to weirdly split this thing up \n",
    "def split_column(row):  \n",
    "    return pd.Series([row[0][0], row[1]])  \n",
    "df1[['state', 'Corr']] = df1[0].apply(split_column)\n",
    "df1.drop(0, axis=1, inplace=True) # Drop the df in place. dont assign to another var\n",
    "df1[['Measure1','Measure2']] = df1[1].apply(lambda x: pd.Series(x)) # This splits the string up for us and such\n",
    "empty_cells = df1.isna() # Are there any na values\n",
    "empty_cells_count = empty_cells.sum()\n",
    "print(empty_cells_count)\n",
    "empty_rows = df1[df1.isna().any(axis=1)]\n",
    "df2 = df1[~df1.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values \n",
    "df2.drop(1,axis=1, inplace=True)\n",
    "len(df2.Corr.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = df2[df2['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "aa=defg[defg['state']=='NC']\n",
    "len(aa['Measure1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 =defg.groupby('state')['Measure1'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df2[~df2.index.isin(defg.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts2 =cleaned_df.groupby('state')['Measure1'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = df2[df2['Corr'] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ah['Measure1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab=df2[df2['state']=='NC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(defg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation values are good\n",
    "\n",
    "defg['Measure2'].value_counts() # Sorting in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3=df2.sort_values(by=['Corr'])\n",
    "df3.sort_values(['Measure1', 'Measure2'], inplace=True) # Obtaining measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = df3['Corr'].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = df2.loc[5283, 'Corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Combined'] = df3['Measure1'].astype(str) + '_' + df3['Measure2'].astype(str) # Combining strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df3.groupby('Combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = grouped_df['Corr'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values_reset = mean_values.reset_index()\n",
    "mean_values_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values_reset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series = mean_values[(mean_values < 0.5) & (mean_values > -0.5)] # Filter all of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series=filtered_series.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_to_drop=(mean_values_reset[~mean_values_reset.Combined.isin(filtered_series.Combined)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stuff_to_drop['Combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_to_keep=(df3[~df3.Combined.isin(stuff_to_drop.Combined)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_to_keep['Combined'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = stuff_to_keep.groupby('state') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
