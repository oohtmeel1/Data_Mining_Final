{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So given the original question: Do states with more places data have different metrics in general? (maybe experience more or less year to year change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "from distfit import distfit\n",
    "import statsmodels.api as sm \n",
    "import pylab\n",
    "from numpy import cov\n",
    "from itertools import permutations \n",
    "import itertools\n",
    "from scipy.stats import spearmanr\n",
    "from random import sample \n",
    "import random \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os  # Import the os module\n",
    "from scipy.stats import iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Background, Loading, Inspection\n",
    "\n",
    "\n",
    "There are many ways to acess the files that are needed. Either downloading from github, or if the files are too big  we can use the shortcuts themselves. And download right from the source. \n",
    "The files can be found here below. \n",
    "\n",
    "\n",
    "https://catalog.data.gov/dataset/places-local-data-for-better-health-place-data-2022-release/resource/4bfea8ab-534b-4f2b-9cb1-d0b951709c2a \n",
    "\n",
    "https://catalog.data.gov/dataset/places-local-data-for-better-health-place-data-2021-release-06a9b/resource/f9bb4b0d-7db2-432e-bf3e-58bd400a6ffc \n",
    "\n",
    "https://catalog.data.gov/dataset/places-local-data-for-better-health-place-data-2020-release-670b7/resource/b95272f6-c03d-487b-81fa-460d4b9bcb1f \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os  # Import the os module\n",
    "\n",
    "print('Downloading started')\n",
    "url = 'https://github.com/oohtmeel1/Data_Mining_Final/raw/main/s2020.zip'\n",
    "\n",
    "# Downloading the file by sending the request to the URL\n",
    "req = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if req.status_code == 200:\n",
    "    # Create a BytesIO object from the content\n",
    "    zip_content = io.BytesIO(req.content)\n",
    "\n",
    "    # Get the current working directory\n",
    "    current_directory = os.getcwd()\n",
    "    print(f'Current Working Directory: {current_directory}')\n",
    "\n",
    "    # Specify the directory where you want to extract the contents\n",
    "    extraction_path = 'zipped_files_now'  # Replace with your desired path\n",
    "    zip_ref = zipfile.ZipFile(zip_content, 'r')\n",
    "\n",
    "    # Create the extraction directory if it doesn't exist\n",
    "    os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "    # Join the extraction path and the filename\n",
    "    extraction_file_path = os.path.join(extraction_path, 's2020.zip')\n",
    "\n",
    "    # Extract the contents\n",
    "    zip_ref.extractall(extraction_file_path)\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_path = 'zipped_files_now'\n",
    "# Specify the filename you want to load\n",
    "csv_filename = 's2020.csv'  # Replace with the actual CSV file name\n",
    "\n",
    "# Join the extraction path and the filename\n",
    "csv_file_path = os.path.join(extraction_path, csv_filename)\n",
    "\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "s2020 = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://data.cdc.gov/api/views/q8ig-wwk9/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "# Downloading the file by sending the request to the URL\n",
    "req = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if req.status_code == 200:\n",
    "    # Create a BytesIO object from the content\n",
    "    s2021 = pd.read_csv(io.StringIO(req.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = ' https://data.cdc.gov/api/views/epbn-9bv3/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "# Downloading the file by sending the request to the URL\n",
    "req = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if req.status_code == 200:\n",
    "    # Create a BytesIO object from the content\n",
    "    s2022 = pd.read_csv(io.StringIO(req.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s2020 =pd.read_csv(\"C:/Users/amcfa/Downloads/PLACES__Local_Data_for_Better_Health__Place_Data_2020_release.csv\")\n",
    "#s2021 =pd.read_csv(\"C:/Users/amcfa/Downloads/PLACES__Local_Data_for_Better_Health__Place_Data_2021_release.csv\")\n",
    "#s2022 =pd.read_csv(\"C:/Users/amcfa/Downloads/PLACES__Local_Data_for_Better_Health__Place_Data_2022_release.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s2020= s2020.drop(['Data Value Footnote','Short Question Text','Category','Data Value Footnote Symbol','StateDesc','DataValueTypeID','CategoryID','MeasureId'],axis=1)\n",
    "#s2021= s2021.drop(['Data Value Footnote','Short_Question Text','Category','Data Value Footnote Symbol','StateDesc','DataValueTypeID','CategoryID','MeasureId'],axis=1)\n",
    "#s2022= s2022.drop(['Data Value Footnote','Short_Question Text','Category','Data Value Footnote Symbol','StateDesc','DataValueTypeID','CategoryID','MeasureId'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2021.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2022.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Basic Data Cleaning\n",
    "All NA values are dropped. And all rows and columns have been inspected to ensure correctness.The only null values remaining are data value footnotes(Which are NaN) but are of no importance. Otherwise the Data Value Column does not containy any null values. And looking at the info shows there are no very large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA values\n",
    "s2020['Measure'] = s2020['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2021['Measure'] = s2021['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2022['Measure'] = s2022['Measure'].str.replace(\" \", \"_\") # Strip whitespace and replace with _\n",
    "s2020= s2020[s2020['Data_Value'].notna()]\n",
    "s2021= s2021[s2021['Data_Value'].notna()]\n",
    "s2022= s2022[s2022['Data_Value'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(s2020['Data_Value'])) # Returns where the dataframe is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(s2021['Data_Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(s2022['Data_Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020.loc[s2020['Data_Value'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2021.loc[s2021['Data_Value'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2022.loc[s2022['Data_Value'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with such a large amounts of data can be a little bit unwieldy at first. Looking over box plots or histograms might become overwhelming. \n",
    "Instead it would be better to use some Python functions instead.\n",
    "The code below:\n",
    "The below code compiles everything by state and test. Type the name of the state (2 letters) below and a big dataframe will appear. \n",
    "I can use that to run mass correlation analysis. \n",
    "While Exec is not recommended. For this project with 153 different variables, each having 30 different possible measures, it can become extremely unwieldly very fast. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below plot answers the obvious. Do places with more locations measured overall have a higher number of Data points obtained. Yes, the answer is yes. They also look fairly consistent year to year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measure_counts = s2020.groupby(['StateAbbr', 'Measure']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts, dodge=True,legend=False)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measure_counts1 = s2021.groupby(['StateAbbr', 'Measure']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts1, dodge=True,legend=False)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measure_counts3 = s2022.groupby(['StateAbbr', 'Measure']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts3, dodge=True,legend=False)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are values divided up as far as category goes?\n",
    "How are the values divided up as far as Category  goes. There are 3. PREVENT, HLTHOUT, abd UNHBEH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "measure_counts5 = s2020.groupby(['StateAbbr', 'CategoryID']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_counts6 = s2021.groupby(['StateAbbr', 'CategoryID']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_counts7 = s2022.groupby(['StateAbbr', 'CategoryID']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts5, hue ='CategoryID',dodge=True,legend=True)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts6, hue ='CategoryID',dodge=True,legend=True)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x='StateAbbr', y='Count', data=measure_counts7, hue ='CategoryID',dodge=True,legend=True)\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Measures in Each State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting up dataframe code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Getting measures by themselves and creating just a ton of df\n",
    "sa=s2020['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2020.groupby(s2020['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2020 = list(dataframes_dict.values())\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "sa=s2021['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2021.groupby(s2021['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2021 = list(dataframes_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting measures by themselves and creating just a ton of df\n",
    "sa=s2022['StateAbbr'].unique()  # Getting all names of all sates\n",
    "n = len(sa)\n",
    "new_list_of_titles = []\n",
    "dfs = []\n",
    "for i in sa:\n",
    "\tif i not in new_list_of_titles:\n",
    "\t\tnew_list_of_titles.append(i)\n",
    "print(new_list_of_titles)\n",
    "\n",
    "\n",
    "grouped = s2022.groupby(s2022['StateAbbr']) # Group by year\n",
    "for i in new_list_of_titles:\n",
    "\texec(f'{i}=grouped.get_group(\"{i}\")')  # Using f string here to execute code. Getting the groups of the grouped well.groups. and executing based on state. So I make 51 dataframes this way one for each\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Create DataFrames and store them in the dictionary \n",
    "for i in new_list_of_titles:\n",
    "\tdataframes_dict[i] = eval(i)\n",
    "# put all of them into a list so I can maybe iterateover them easier Use this for the iteration\n",
    "#eval(f'dataframes_list_{year} = list(dataframes_dict.values())') # no one sees me using exec. Nope. \n",
    "dataframes_list2022 = list(dataframes_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2020[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2021[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022[0].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs the summary statistics for all unique measures in the entire dataframe by State.\n",
    "\n",
    "And in order to remove outliers we can use the ``` IQR ``` method. Where we set up a 'fence' outside of Q1 and Q3. Anything outside of this fence are considered outliers.\n",
    "\n",
    "The formula for calculating the 'fence' is \n",
    "$$\\begin{align}\n",
    "((1.5 * IQR) - Q1)  \n",
    "\\end{align}$$\n",
    "OR \n",
    "$$\\begin{align}\n",
    "((1.5 * IQR) + Q3) \n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=dataframes_list2020[50]\n",
    "len(abc.loc[abc['Measure']=='Chronic_obstructive_pulmonary_disease_among_adults_aged_>=18_years','Data_Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "#n=2\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\t#print(subseta)\n",
    "\tmatching_indices1 = [index for index, df in enumerate(dataframes_list2020) if (df['StateAbbr'].isin(subseta).any())]# No matter where they are, this could should find the matching states. \n",
    "\tfor index in matching_indices1:\n",
    "\t\tmatches_2020 = dataframes_list2020[index]\n",
    "\t\tsubset1=(matches_2020['Measure'].unique()) \n",
    "\t\tintersection_list = set(list(subset1))\n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\tlis1 = matches_2020.loc[matches_2020['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t#print(len(lis1))\n",
    "\t\t\tmy_array = np.array(lis1)\n",
    "\t\t\tq1,q3 = np.percentile(my_array, (25,75))\n",
    "\t\t\tiqr1 = q3-q1\t\t\n",
    "\t\t\t#print(iqr)\n",
    "\t\t\tmin1 = q1-(1.5*iqr1) # Subtract from Q1\n",
    "\t\t\t#(print(min1))\n",
    "\t\t\tmax1 = (1.5*iqr1)+q3 # Add to Q3\n",
    "\t\t\tx_sum.append([subseta,min1,max1,i,q1,q3])\n",
    "\t\t\t#print(\"Total prints:\", print_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_min_max=pd.DataFrame(x_sum) # Put those in a dataframe\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max[0].astype(str)\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max['StateAbbr'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "List_of_min_max[\"StateAbbr\"] = List_of_min_max[\"StateAbbr\"].str.strip()\n",
    "List_of_min_max['min1'] = List_of_min_max[1].astype(float)\n",
    "List_of_min_max['max1'] = List_of_min_max[2].astype(float)\n",
    "List_of_min_max['Measure'] = List_of_min_max[3].astype(str)\n",
    "List_of_min_max['Measure'] = List_of_min_max['Measure'].str.strip()\n",
    "List_of_min_max['Q1'] = List_of_min_max[4].astype(float)\n",
    "List_of_min_max['Q3'] = List_of_min_max[5].astype(float)\n",
    "List_of_min_max= List_of_min_max.drop([0,1,2,3,4,5],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okee=s2020[s2020['StateAbbr']=='NC']\n",
    "abc2=pd.DataFrame(okee.loc[okee['Measure']=='Older_adult_women_aged_>=65_years_who_are_up_to_date_on_a_core_set_of_clinical_preventive_services:_Flu_shot_past_year,_PPV_shot_ever,_Colorectal_cancer_screening,_and_Mammogram_past_2_years','Data_Value'])\n",
    "abc2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentdf=dataframes_list2020[0] # Manually Checking My data\n",
    "experimentdf1=dataframes_list2020[1] \n",
    "experimentdf2=dataframes_list2020[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentdf1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=s2020.columns.values.tolist()\n",
    "dataframes_list2020_1 =[] #<- New Dataframe list\n",
    "n = len(dataframes_list2020)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "for j in range(n):\n",
    "\n",
    "\tmatches_2020 = dataframes_list2020[j] # <- Change this matches 2020 is first dataframe\n",
    "\tmatches_2020['Measure'] = matches_2020['Measure'].str.strip()\n",
    "\tmatches_2020['StateAbbr']=matches_2020['StateAbbr'].str.strip()\n",
    "\tmatches_2020['Data_Value']=matches_2020['Data_Value'].astype(int)\n",
    "\t#print(matches_2020['StateAbbr'].unique())\n",
    "\txbar = len(matches_2020['Measure'].unique()) # match all of the unique values\n",
    "\tlong_dataframe_list=[]\n",
    "\tfor k in range(xbar):\n",
    "\t\n",
    "\t\tfirst_row = List_of_min_max.iloc[k] # Matches the same index in my new df because they should be the same\n",
    "\t\t#print(first_row)\n",
    "\t\tstate_abbr = first_row[0]\n",
    "\t\t#print(state_abbr)\n",
    "\t\tmeasure = first_row[3]\n",
    "\t\t#print(measure)\n",
    "\t\tmaxes1 = first_row[2].astype(int)\n",
    "\t\t#print(maxes1)\n",
    "\t\tmins1 = first_row[1].astype(int)\n",
    "\t\t#print(mins1)\n",
    "\t\tcondition = matches_2020[matches_2020['Measure']== measure]\n",
    "\t\tfiltering_now = condition[condition['Data_Value'] >= mins1]\n",
    "\t\tfiltering_now1 = filtering_now[filtering_now['Data_Value'] <= maxes1]\n",
    "\t\t#result_df = pd.concat([result_df,filtering_now1])\n",
    "\t\tresult_df1= filtering_now1.values.tolist()\n",
    "\t\tlong_dataframe_list.extend(result_df1)\n",
    "\t\t\n",
    "\tresults_test=pd.DataFrame(long_dataframe_list,columns=column_names)\n",
    "\tdataframes_list2020_1.append(results_test) # Remember to change this after debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2020_1[1].loc[dataframes_list2020_1[1]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of outliers for 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum1=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2021)\n",
    "\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\t#print(subseta)\n",
    "\tmatching_indices1 = [index for index, df in enumerate(dataframes_list2021) if (df['StateAbbr'].isin(subseta).any())]# No matter where they are, this could should find the matching states. \n",
    "\tfor index in matching_indices1:\n",
    "\t\tmatches_2021 = dataframes_list2021[index]\n",
    "\t\tsubset1=(matches_2021['Measure'].unique()) \n",
    "\t\tintersection_list = set(list(subset1))\n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\tlis1 = matches_2021.loc[matches_2021['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t#print(len(lis1))\n",
    "\t\t\tmy_array = np.array(lis1)\n",
    "\t\t\tq1,q3 = np.percentile(my_array, (25,75))\n",
    "\t\t\tiqr1 = q3-q1\t\t\n",
    "\t\t\t#print(iqr)\n",
    "\t\t\tmin1 = q1-(1.5*iqr1) # Subtract from Q1\n",
    "\t\t\t#(print(min1))\n",
    "\t\t\tmax1 = (1.5*iqr1)+q3 # Add to Q3\n",
    "\t\t\tx_sum1.append([subseta,min1,max1,i,q1,q3])\n",
    "\t\t\t#print(\"Total prints:\", print_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_min_max=pd.DataFrame(x_sum1) # Put those in a dataframe\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max[0].astype(str)\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max['StateAbbr'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "List_of_min_max[\"StateAbbr\"] = List_of_min_max[\"StateAbbr\"].str.strip()\n",
    "List_of_min_max['min1'] = List_of_min_max[1].astype(float)\n",
    "List_of_min_max['max1'] = List_of_min_max[2].astype(float)\n",
    "List_of_min_max['Measure'] = List_of_min_max[3].astype(str)\n",
    "List_of_min_max['Measure'] = List_of_min_max['Measure'].str.strip()\n",
    "List_of_min_max['Q1'] = List_of_min_max[4].astype(float)\n",
    "List_of_min_max['Q3'] = List_of_min_max[5].astype(float)\n",
    "List_of_min_max= List_of_min_max.drop([0,1,2,3,4,5],axis=1)\n",
    "List_of_min_max_2021 =List_of_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=s2021.columns.values.tolist()\n",
    "dataframes_list2021_1 =[] #<- New Dataframe list\n",
    "n = len(dataframes_list2021)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "for j in range(n):\n",
    "\n",
    "\tmatches_2021 = dataframes_list2021[j] # <- Change this matches 2020 is first dataframe\n",
    "\tmatches_2021['Measure'] = matches_2021['Measure'].str.strip()\n",
    "\tmatches_2021['StateAbbr']=matches_2021['StateAbbr'].str.strip()\n",
    "\tmatches_2021['Data_Value']=matches_2021['Data_Value'].astype(int)\n",
    "\t#print(matches_2020['StateAbbr'].unique())\n",
    "\txbar = len(matches_2021['Measure'].unique()) # match all of the unique values\n",
    "\tlong_dataframe_list=[]\n",
    "\tfor k in range(xbar):\n",
    "\t\n",
    "\t\tfirst_row = List_of_min_max_2021.iloc[k] # Matches the same index in my new df because they should be the same\n",
    "\t\t#print(first_row)\n",
    "\t\tstate_abbr = first_row[0]\n",
    "\t\t#print(state_abbr)\n",
    "\t\tmeasure = first_row[3]\n",
    "\t\t#print(measure)\n",
    "\t\tmaxes1 = first_row[2].astype(int)\n",
    "\t\t#print(maxes1)\n",
    "\t\tmins1 = first_row[1].astype(int)\n",
    "\t\t#print(mins1)\n",
    "\t\tcondition = matches_2021[matches_2021['Measure']== measure]\n",
    "\t\tfiltering_now = condition[condition['Data_Value'] >= mins1]\n",
    "\t\tfiltering_now1 = filtering_now[filtering_now['Data_Value'] <= maxes1]\n",
    "\t\t#result_df = pd.concat([result_df,filtering_now1])\n",
    "\t\tresult_df1= filtering_now1.values.tolist()\n",
    "\t\tlong_dataframe_list.extend(result_df1)\n",
    "\t\t\n",
    "\tresults_test=pd.DataFrame(long_dataframe_list,columns=column_names)\n",
    "\tdataframes_list2021_1.append(results_test) # Remember to change this after debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2021_1[0]['Measure'].nunique()\n",
    "dataframes_list2021_1[0].loc[dataframes_list2021_1[0]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2021[0].loc[dataframes_list2021[0]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of outliers 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum2=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2022)\n",
    "\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\t#print(subseta)\n",
    "\tmatching_indices1 = [index for index, df in enumerate(dataframes_list2022) if (df['StateAbbr'].isin(subseta).any())]# No matter where they are, this could should find the matching states. \n",
    "\tfor index in matching_indices1:\n",
    "\t\tmatches_2022 = dataframes_list2022[index]\n",
    "\t\tsubset1=(matches_2022['Measure'].unique()) \n",
    "\t\tintersection_list = set(list(subset1))\n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\tlis1 = matches_2022.loc[matches_2022['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\t#print(len(lis1))\n",
    "\t\t\tmy_array = np.array(lis1)\n",
    "\t\t\tq1,q3 = np.percentile(my_array, (25,75))\n",
    "\t\t\tiqr1 = q3-q1\t\t\n",
    "\t\t\t#print(iqr)\n",
    "\t\t\tmin1 = q1-(1.5*iqr1) # Subtract from Q1\n",
    "\t\t\t#(print(min1))\n",
    "\t\t\tmax1 = (1.5*iqr1)+q3 # Add to Q3\n",
    "\t\t\tx_sum2.append([subseta,min1,max1,i,q1,q3])\n",
    "\t\t\t#print(\"Total prints:\", print_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_min_max=pd.DataFrame(x_sum2) # Put those in a dataframe\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max[0].astype(str)\n",
    "List_of_min_max['StateAbbr'] = List_of_min_max['StateAbbr'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "List_of_min_max[\"StateAbbr\"] = List_of_min_max[\"StateAbbr\"].str.strip()\n",
    "List_of_min_max['min1'] = List_of_min_max[1].astype(float)\n",
    "List_of_min_max['max1'] = List_of_min_max[2].astype(float)\n",
    "List_of_min_max['Measure'] = List_of_min_max[3].astype(str)\n",
    "List_of_min_max['Measure'] = List_of_min_max['Measure'].str.strip()\n",
    "List_of_min_max['Q1'] = List_of_min_max[4].astype(float)\n",
    "List_of_min_max['Q3'] = List_of_min_max[5].astype(float)\n",
    "List_of_min_max= List_of_min_max.drop([0,1,2,3,4,5],axis=1)\n",
    "List_of_min_max_2022 =List_of_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=s2022.columns.values.tolist()\n",
    "dataframes_list2022_1 =[] #<- New Dataframe list\n",
    "n = len(dataframes_list2022)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "for j in range(n):\n",
    "\n",
    "\tmatches_2022 = dataframes_list2022[j] # <- Change this matches 2020 is first dataframe\n",
    "\tmatches_2022['Measure'] = matches_2022['Measure'].str.strip()\n",
    "\tmatches_2022['StateAbbr']=matches_2022['StateAbbr'].str.strip()\n",
    "\tmatches_2022['Data_Value']=matches_2022['Data_Value'].astype(int)\n",
    "\t#print(matches_2020['StateAbbr'].unique())\n",
    "\txbar = len(matches_2022['Measure'].unique()) # match all of the unique values\n",
    "\tlong_dataframe_list=[]\n",
    "\tfor k in range(xbar):\n",
    "\t\n",
    "\t\tfirst_row = List_of_min_max_2022.iloc[k] # Matches the same index in my new df because they should be the same\n",
    "\t\t#print(first_row)\n",
    "\t\tstate_abbr = first_row[0]\n",
    "\t\t#print(state_abbr)\n",
    "\t\tmeasure = first_row[3]\n",
    "\t\t#print(measure)\n",
    "\t\tmaxes1 = first_row[2].astype(int)\n",
    "\t\t#print(maxes1)\n",
    "\t\tmins1 = first_row[1].astype(int)\n",
    "\t\t#print(mins1)\n",
    "\t\tcondition = matches_2022[matches_2022['Measure']== measure]\n",
    "\t\tfiltering_now = condition[condition['Data_Value'] >= mins1]\n",
    "\t\tfiltering_now1 = filtering_now[filtering_now['Data_Value'] <= maxes1]\n",
    "\t\t#result_df = pd.concat([result_df,filtering_now1])\n",
    "\t\tresult_df1= filtering_now1.values.tolist()\n",
    "\t\tlong_dataframe_list.extend(result_df1)\n",
    "\t\t\n",
    "\tresults_test=pd.DataFrame(long_dataframe_list,columns=column_names)\n",
    "\tdataframes_list2022_1.append(results_test) # Remember to change this after debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022_1[0]['Measure'].nunique()\n",
    "dataframes_list2022_1[0].loc[dataframes_list2022_1[0]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022[0].loc[dataframes_list2022[0]['Measure']=='Current_smoking_among_adults_aged_>=18_years','Data_Value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2022_1[50].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Cells seem normal. The values update accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list2020 = dataframes_list2020_1\n",
    "dataframes_list2021 = dataframes_list2021_1\n",
    "dataframes_list2022 = dataframes_list2022_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Code as of 11/08/2023 - this code works (For 2020)\n",
    "Lets Hope This code works right, and it does At least it is correct otherwise(I Checked for unique Corr values and there were almost as many as the length of the list)\n",
    "I needed correlation Values for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2020[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\tif len(lis1) != len(lis2):\n",
    "\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\tif len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\t\t#print(stats.spearmanr(lis1, lis2),i)\n",
    "\t\tx.append([subsetb,subsetc,stats.spearmanr(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\tresult1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['state'] = df1[0]+df1[1]\n",
    "df1['state'] = df1['state'].astype(str)\n",
    "df1['result_string'] = df1[2].astype(str) # gotta change the kurkwalis to string\n",
    "df1['Corr'] = df1['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df1['p-value'] = df1['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df1['State1'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df1['State2'] = df1[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df1= df1.drop(['result_string',0,1,2],axis=1)\n",
    "df1['p-value'] = df1['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df1['p-value'] = pd.to_numeric(df1['p-value'], errors='coerce') # Convert P value to numeric\n",
    "df1['Measure1'] = df1[3]\n",
    "empty_rows = df1[df1.isna().any(axis=1)]\n",
    "df1 = df1[~df1.index.isin(empty_rows.index)] # Use boolean indexing to get rid of all of the empty values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So this Value below can be used to select all of the state combinations that pass correlation test 11/15/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df1[df1['Corr'] < 0.5]\n",
    "filtered_df1 = filtered_df[filtered_df[\"Corr\"]>-0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df1.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(dataframes_list2021)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2021[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[]\n",
    "y=[]\n",
    "\n",
    "n = len(dataframes_list2021)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2021[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2021[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\tif len(lis1) != len(lis2):\n",
    "\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\tif len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\t\t#print(stats.spearmanr(lis1, lis2),i)\n",
    "\t\tx1.append([subsetb,subsetc,stats.spearmanr(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(x1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['state'] = df2[0]+df2[1]\n",
    "df2['state'] = df2['state'].astype(str)\n",
    "df2['result_string'] = df2[2].astype(str) # gotta change the kurkwalis to string\n",
    "df2['Corr'] = df2['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df2['p-value'] = df2['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df2['State1'] = df2[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df2['State2'] = df2[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df2= df2.drop(['result_string',0,1,2],axis=1)\n",
    "df2['p-value'] = df2['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df2['p-value'] = pd.to_numeric(df2['p-value'], errors='coerce') # Convert P value to numeric\n",
    "df2['Measure1'] = df2[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df2[df2['Corr'] < 0.5]\n",
    "filtered_df2 = filtered_df[filtered_df[\"Corr\"]>-0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df2.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(dataframes_list2022)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2022[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=[]\n",
    "y=[]\n",
    "\n",
    "n = len(dataframes_list2022)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2022[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2022[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\tif len(lis1) != len(lis2):\n",
    "\t\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\tif len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\tif len(lis2) == min_len:\n",
    "\t\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\t\t#print(stats.spearmanr(lis1, lis2),i)\n",
    "\t\tx2.append([subsetb,subsetc,stats.spearmanr(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(x2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['state'] = df3[0]+df3[1]\n",
    "df3['state'] = df3['state'].astype(str)\n",
    "df3['result_string'] = df3[2].astype(str) # gotta change the kurkwalis to string\n",
    "df3['Corr'] = df3['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df3['p-value'] = df3['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df3['State1'] = df3[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df3['State2'] = df3[1].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df3= df3.drop(['result_string',0,1,2],axis=1)\n",
    "df3['p-value'] = df3['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df3['p-value'] = pd.to_numeric(df3['p-value'], errors='coerce') # Convert P value to numeric\n",
    "df3['Measure1'] = df3[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df3[df3['Corr'] < 0.5]\n",
    "filtered_df3 = filtered_df[filtered_df[\"Corr\"]>-0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = filtered_df3.groupby('State2') # So group each by state\n",
    "fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After data is collected it can now be used for visualization and such (11/08/2023) Also not needed\n",
    "\n",
    "Well I can now see if states with higher counts of PLACES data have different correlation values. Why not. First I need to check for some empty values. And there were just a few. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1 = pd.DataFrame(result1)\n",
    "df3 = pd.DataFrame(result2)\n",
    "df5 = pd.DataFrame(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/08/2023\n",
    "Maybe the first correlation Value I can look at is the overall max cor value for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a cool little form of df groupby kind of neat. Uses idxmax, which returns the max of each index\n",
    "def max_corr(group):\n",
    "    max_corr_idx = group['Corr'].idxmax()\n",
    "    return group.loc[max_corr_idx,['Corr','Measure1','Measure2']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result1 = df2.groupby('state').apply(max_corr).reset_index()  # Groupby state\n",
    "result2 = df4.groupby('state').apply(max_corr).reset_index()  # Groupby state\n",
    "result3 = df6.groupby('state').apply(max_corr).reset_index()  # Groupby state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/08/2023\n",
    "\n",
    "So viewing the results briefly, Chronic kidney disease and stroke have some correlation in this data set. And I notice the correlation values for the other two years are much much lower. It is strange. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So given the original question: Do states with more places data have higher metrics in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the counts for all of the places data divided by state, PA, TX, CA, IL, have the most by a wide margin so let's look at those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=s2020[s2020['StateAbbr']=='NC']\n",
    "m['LocationName'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "years = ['2020', '2021', '2022']\n",
    "values = [28329, 28329, 28329]\n",
    "\n",
    "\n",
    "plt.bar(years, values, color='green')\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Sum of all cities in PLACES Dataset per year')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "city_counts2 = s2021.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts2 = city_counts2.sort_values(ascending=False)\n",
    "city_counts3 = s2022.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts3 = city_counts3.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['PA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1['TX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"dark:#5A9_r\")\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts2,palette=\"dark:#5A9_r\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts3,palette=\"dark:#5A9_r\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No matter what Use the cor analysis to select the rows or columns (whatever) that have little or no correlation and run statistical analysis \n",
    "Year to year there is a decrease in correlation between variables. The first year there is a wide range of correlation values, and the following years there is almost none. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And looking at mean and median values for correlations for each year and set of states, all show a similiar decrease in correlation values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations seem fairly similiar. ## Big Blob from the bottom where correlation analysis was done will go here\n",
    "\n",
    "Now time to look at actual data itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = df1[df1['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "\n",
    "city_counts1 =defg.groupby('state')['Measure1'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = df1[df1['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "\n",
    "city_counts1 =defg.groupby('state')['Measure1'].nunique()\n",
    "cleaned_df = df1[df1.index.isin(defg.index)]\n",
    "city_counts2 =cleaned_df.groupby('state')['Measure1'].nunique()\n",
    "ah1 = df1[df1['Corr'] > 0.5]\n",
    "len(ah1['Measure1'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Working down here for correlations as of 11/15/2023\n",
    "\n",
    "#Trying to organize my values and decide if the values should be dropped. The below code filtered out all of the combinations that had corr values that were not significant. As far as how to move forward. Not sure\n",
    "# I had mixed data in the column so I needed to weirdly split this thing up \n",
    "\n",
    "\n",
    "abcd = df2[df2['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "\n",
    "city_counts1 =defg.groupby('state')['Measure1'].nunique()\n",
    "cleaned_df2 = df2[df2.index.isin(defg.index)]\n",
    "city_counts2 =cleaned_df2.groupby('state')['Measure1'].nunique()\n",
    "ah2 = df2[df2['Corr'] > 0.5]\n",
    "len(ah2['Measure1'].unique())\n",
    "\n",
    "\n",
    "\n",
    "# Correlation values are good\n",
    "\n",
    "len(cleaned_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = df3[df3['Corr'] < 0.5]\n",
    "defg=abcd[abcd['Corr'] >-0.5]\n",
    "\n",
    "city_counts3 =defg.groupby('state')['Measure1'].nunique()\n",
    "cleaned_df3 = df3[df3.index.isin(defg.index)]\n",
    "city_counts3 =cleaned_df3.groupby('state')['Measure1'].nunique()\n",
    "ah3 = df3[df3['Corr'] > 0.5]\n",
    "len(ah3['Measure1'].unique())\n",
    "len(cleaned_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''defg['Measure2'].value_counts() # Sorting in place\n",
    "\n",
    "df3=df2.sort_values(by=['Corr'])\n",
    "df3.sort_values(['Measure1', 'Measure2'], inplace=True) # Obtaining measures\n",
    "mean_values = df3['Corr'].mean\n",
    "\n",
    "df3['Combined'] = df3['Measure1'].astype(str) + '_' + df3['Measure2'].astype(str) # Combining strings\n",
    "grouped_df = df3.groupby('Combined')\n",
    "mean_values = grouped_df['Corr'].median()\n",
    "mean_values '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mean_values_reset = mean_values.reset_index()\n",
    "mean_values_reset \n",
    "filtered_series = mean_values[(mean_values < 0.5) & (mean_values > -0.5)] # Filter all of the values. \n",
    "filtered_series=filtered_series.reset_index()\n",
    "stuff_to_drop=(mean_values_reset[~mean_values_reset.Combined.isin(filtered_series.Combined)])\n",
    "print(stuff_to_drop['Combined'])\n",
    "stuff_to_keep=(df3[~df3.Combined.isin(stuff_to_drop.Combined)])\n",
    "stuff_to_keep['Combined'].nunique()\n",
    "grouped = stuff_to_keep.groupby('state') # So group each by state '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''fig, axes = plt.subplots(nrows=11, ncols=5, figsize=(50, 55))  # Adjust figsize as needed\n",
    "for i, (state, group) in enumerate(grouped):\n",
    "    ax = axes[i // 5, i % 5]  # Calculate the correct subplot based on row and column ( a 10 by 5 basically with extra steps)\n",
    "    ax.hist(group['Corr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of Corr Values for {state}')\n",
    "    ax.set_xlabel('Corr Value')\n",
    "    ax.set_ylabel('Frequency')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big blob for correlation analysis will stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below code tests for normality using a D’Agostino’s K^2 test. \n",
    "\n",
    "But just because the data is not normally distributed, does not mean it is Gaussian in shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "alpha = 0.05\n",
    "state= dataframes_list2020[0]\n",
    "subset1=(state['Measure'].unique())\n",
    "for i in subset1:\n",
    "\tlis1 = state.loc[state['Measure']==i,'Data_Value']\n",
    "\tstat, p = normaltest(lis1)\n",
    "\tprint('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\tif p > alpha:\n",
    " \t\tprint('Sample looks Gaussian (fail to reject H0)')\n",
    "\telse:\n",
    " \t\tprint('Sample is not Gaussian, maybe normal (reject H0)')\n",
    "\t\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originally the data was processed using the fitdist function.\n",
    "In order to define the best fit distribution of all data, to see which statistical test to employ for each state, and metric for that state. \n",
    "\n",
    "And Now I am going to use R to help me work with this data. R has great statistical packages to work with just means of data for example. Also there are few sets of data that fit into a normal distribution so I will use nonparametric tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1= pd.read_csv('2020_data_df')  # Read in the data\n",
    "df2= pd.read_csv('2021_data_df')\n",
    "df3= pd.read_csv('2022_data_df')\n",
    "trying2020 = df1[['State','Measure_Name','Best_Fit_Distribution','mean']] # Select only this subset\n",
    "trying2021 = df2[['State','Measure_Name','Best_Fit_Distribution','mean']]\n",
    "trying2022 = df3[['State','Measure_Name','Best_Fit_Distribution','mean']]\n",
    "#norm2020 = trying2020[trying2020['Best_Fit_Distribution']=='norm'] # Further Filter each DF\n",
    "#norm2021 = trying2021[trying2021['Best_Fit_Distribution']=='norm']\n",
    "#norm2022 = trying2022[trying2022['Best_Fit_Distribution']=='norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying2020 =trying2020[~trying2020['Best_Fit_Distribution'].str.contains('norm')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying2021 =trying2021[~trying2021['Best_Fit_Distribution'].str.contains('norm')]\n",
    "#trying2022 =trying2022[~trying2022['Best_Fit_Distribution'].str.contains('norm')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any normally distributed data I can test using the paired t test\n",
    "to start maybe split all of the dat on these lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying2020[['group', 'year', 'testnumber']] = trying2020['State'].apply(lambda x: pd.Series(str(x).split('_'))) # apply seems to force the data to split\n",
    "trying2021[['group', 'year', 'testnumber']] = trying2021['State'].apply(lambda x: pd.Series(str(x).split('_')))\n",
    "trying2022[['group', 'year', 'testnumber']] = trying2022['State'].apply(lambda x: pd.Series(str(x).split('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/09/2023\n",
    "Now group again by state and measure. grouped list of df,then group by measure and then compare year to year. And this is difficult because I cant just merge the DF because of their size. I would run into a memory error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is trying to go by the dataframes in place kind of. It finds the first match, then matches in the others. While it is not as fast it should hopefully not be super computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working On code down here 11/09/2023 The code below is for all Data \n",
    "\n",
    "The Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs. Post hoc comparisons between groups are required to determine which groups are different.*\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html\n",
    "\n",
    "\n",
    "For the code below. I take each state index, match the same states in the other dataframes and then run all stats kruskal tests on the variables. This test can also take in arrays of different lengths so I commented out the random sampling to make them the same. \n",
    "\n",
    "I need to see if the medians of the groups are rejected more for the cities with more places data than others. Over years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As of 11/15/2023 worked here on excluding highly correlated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below shortens the result dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above gives all results where there are no year to year changes. And it seems all states change year to year when compared to themselves. So How about just the biggest cities states tested for differences against all other states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/09/20203 The below code Compares texas to all other states for all metrics for the year 2020. \n",
    "If my lists are not the same length I need to do some repetition and take the averages no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2020[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t#else:\n",
    "\t\t\t\t#continue\n",
    "\t\t#print(stats.kruskal(lis1, lis2),i)\n",
    "\t\tx.append([subsetb,subsetc,stats.kruskal(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\tresult1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1['result_string'] = df1[0].astype(str) # gotta change the kurkwalis to string\n",
    "df1['statistic'] = df1['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df1['p-value'] = df1['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df1['State'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df1['state'] = df1[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ''.join(x[0]))\n",
    "df1[['State1','State2']]=df1['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df1= df1.drop(['result_string',0],axis=1)\n",
    "df1['p-value'] = df1['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df1['p-value'] = pd.to_numeric(df1['p-value'], errors='coerce') # Convert P value to numeric\n",
    "#filtered_df = df2[df2['p-value'] < 0.05]\n",
    "df1['Measure']= df1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_2020 = df1.loc[cleaned_df.index]\n",
    "result_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 texas code stops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021 texas code starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2021)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2021 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2021[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2021)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2021 = dataframes_list2021[j] # matches 2021 is first dataframe\n",
    "\tsubseta=(matches_2021['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2021[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2021[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\ty.append([i])\n",
    "\t\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\t#print(min_len)\n",
    "\t\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\t\n",
    "\t\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t\t#print(lis2)\n",
    "\t\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t#else:\n",
    "\t\t\t\t\t#continue\n",
    "\t\t\t#print(stats.kruskal(lis1, lis2),i)\n",
    "\t\t\tx.append([subsetb,subsetc,stats.kruskal(lis1, lis2),i])\n",
    "\t\t\tprint_count += 1\n",
    "\t\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t\tresult2 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.DataFrame(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['result_string'] = df3[0].astype(str) # gotta change the kurkwalis to string\n",
    "df3['statistic'] = df3['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df3['p-value'] = df3['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df3['State'] = df3[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df3['state'] = df3[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ''.join(x[0]))\n",
    "df3[['State1','State2']]=df3['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df3= df3.drop(['result_string',0],axis=1)\n",
    "df3['p-value'] = df3['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df3['p-value'] = pd.to_numeric(df3['p-value'], errors='coerce') # Convert P value to numeric\n",
    "df3['Measure']= df3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Measure'] = df3[1].astype(str)\n",
    "df3['Measure'] = df3['Measure'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "df3= df3.drop([1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df2['Measure'] = filtered_df2[3].astype(str)\n",
    "filtered_df2['Measure'] = filtered_df2['Measure'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "filtered_df2= filtered_df2.drop([3],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2['state'] = filtered_df2['state'].astype(str)\n",
    "filtered_df2['state'] = filtered_df2['state'].str.replace(\"['\", '').str.replace(\"']\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2021 = df3.merge(filtered_df2, how='inner', on=['Measure','state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of 2021 texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start  of 2022 texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2022)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2021 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2022[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2022)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2022 = dataframes_list2022[j] # matches 2021 is first dataframe\n",
    "\tsubseta=(matches_2022['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2022[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2022[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\t\tfor i in intersection_list:\n",
    "\t\t\ty.append([i])\n",
    "\t\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t\t#print(min_len)\n",
    "\t\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\t\n",
    "\t\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t\t#print(lis2)\n",
    "\t\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t\t#else:\n",
    "\t\t\t\t\t#continue\n",
    "\t\t\t#print(stats.kruskal(lis1, lis2),i)\n",
    "\t\t\tx.append([subsetb,subsetc,stats.kruskal(lis1, lis2),i])\n",
    "\t\t\tprint_count += 1\n",
    "\t\t\t#print(\"Total prints:\", print_count)\n",
    "\t\t\tresult3 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame(result3)\n",
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['result_string'] = df4[0].astype(str) # gotta change the kurkwalis to string\n",
    "df4['statistic'] = df4['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df4['p-value'] = df4['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df4['State'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df4['state'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ''.join(x[0]))\n",
    "df4[['State1','State2']]=df4['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df4= df4.drop(['result_string',0],axis=1)\n",
    "df4['p-value'] = df4['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df4['p-value'] = pd.to_numeric(df4['p-value'], errors='coerce') # Convert P value to numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Measure'] = df4[1].astype(str)\n",
    "df4['Measure'] = df4['Measure'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "df4= df4.drop([1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3= filtered_df3.rename(columns={3:\"Measure\"})\n",
    "filtered_df3['state'] = filtered_df3['state'].astype(str)\n",
    "filtered_df3['state'] = filtered_df3['state'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "filtered_df3['Measure'] = filtered_df3['Measure'].str.strip()\n",
    "filtered_df3['state'] = filtered_df3['state'].str.strip()\n",
    "df4[\"Measure\"] = df4[\"Measure\"].str.strip()\n",
    "df4[\"state\"] = df4[\"state\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2022 = df4.merge(filtered_df3, how='inner', on=['Measure','state'])\n",
    "result_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/10/2023\n",
    "So to see if Texas for example differed more than any other state, I need to sum all of the times texas occurs and then sum all of the other states. The above filtered DF is where there is a difference from the median. And if texas occurs more, then.. it occurs more. It does not answer the question on if the data is higher or lower. Just that a difference exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df['State1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df['State1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df['State2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered_df1['State2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of the results are overwhelmingly, yes the values are different. Post Hoc analysis must be performed.The dunn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of the Dunn test \n",
    "\n",
    "Since the data are mostly nonparametric. I can only say if they are different from each other, and not whether one is higher or lower. But if the sample sizes are large enough normality might be able to be assumed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\t\t\n",
    "\t\tprint(\"Ahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/10/2023\n",
    "Okay I will now Work through each DF, and gather all of the data I need. My goal is to waste as little space as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_measures1 = s2020['Measure'].unique().tolist() # Put all unique values in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_measures1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugh = [] # New  Empty List\n",
    "state_names = []\n",
    "\n",
    "list_of_measures1 = s2020['Measure'].unique().tolist() # Put all unique values in a list\n",
    "\n",
    "\n",
    "result1 = s2020[s2020['Measure'] == 'Cancer_(excluding_skin_cancer)_among_adults_aged_>=18_years'] # Go over each measure\n",
    "for i in new_list_of_titles:\n",
    "\tcondition = lambda x: x['StateAbbr'] == i  # Gather all of the state data\n",
    "\tfiltered_data = result1[result1.apply(condition, axis=1)] # Filter all of the state data\n",
    "\tlis1 = filtered_data['Data_Value'].tolist()\n",
    "\tugh.append(lis1)\n",
    "\tstate_names.append(i)\n",
    "ab= sp.posthoc_dunn(ugh, p_adjust = 'bonferroni')\n",
    "df_dunn_result = pd.DataFrame(ab, index=ab.index, columns=ab.columns) # in order to get the data to appear appropriately, need to include index and columns\n",
    "filtered_df5=df_dunn_result.where(df_dunn_result<0.05)\n",
    "indices = [(l, m) for l, row in enumerate(filtered_df5.values) for m, value in enumerate(row) if pd.notna(value)]\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mapping = {index: state_names[i] for i, index in enumerate(range(len(state_names)))} # Enumerates the state names list\n",
    "state_pairs = [(state_mapping[i], state_mapping[j]) for i, j in indices] # Maps the state names listto its respective index. \n",
    "sorting_pairs = set([tuple(sorted(pair)) for pair in state_pairs]) # Okay so sort the things and make them into a set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_dunn_TX=(df_dunn_result[43].where(df_dunn_result[43]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_TX=only_dunn_TX.dropna()\n",
    "len(only_dunn_TX)\n",
    "only_dunn_CA=(df_dunn_result[4].where(df_dunn_result[4]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_CA=only_dunn_CA.dropna()\n",
    "len(only_dunn_CA)\n",
    "only_dunn_PA=(df_dunn_result[38].where(df_dunn_result[38]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_PA=only_dunn_PA.dropna()\n",
    "len(only_dunn_PA)\n",
    "only_dunn_IL=(df_dunn_result[12].where(df_dunn_result[12]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_IL=only_dunn_IL.dropna()\n",
    "len(only_dunn_IL)\n",
    "only_dunn_SD=(df_dunn_result[41].where(df_dunn_result[41]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_SD=only_dunn_SD.dropna()\n",
    "len(only_dunn_SD)\n",
    "only_dunn_WY=(df_dunn_result[50].where(df_dunn_result[50]<0.05)) # Now we have the Results for this one column. And I will sort by p value less than 0.05\n",
    "only_dunn_WY=only_dunn_WY.dropna()\n",
    "len(only_dunn_WY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(only_dunn_TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/11/2023 Sorted all of the state counts\n",
    "So using the above code and visualizing it. It is easy to see that Over all Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to count rows and columns separately. \n",
    "rows, columns = zip(*indices)\n",
    "\n",
    "# Count occurrences of each row and column index\n",
    "row_counts = Counter(rows)\n",
    "column_counts = Counter(columns)\n",
    "dataframe_for_rows=pd.DataFrame.from_dict(row_counts, orient='index', columns=['Count'])\n",
    "dataframe_for_rows['States']= [state_names[row] for row in dataframe_for_rows.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows1=pd.DataFrame.from_dict(row_counts, orient='index', columns=['Count'])\n",
    "# Now I have to calculate the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows2 = dataframe_for_rows.set_index(['States'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm=dataframe_for_rows2.T\n",
    "print(hm.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_for_rows2['calculated'] = dataframe_for_rows2['Count'].apply(lambda x: 'yes' if x < 46 else 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe_for_rows2[dataframe_for_rows2['calculated']=='yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(hm,palette=\"cividis\")\n",
    "\n",
    "ax.set(xlabel=\"States\",ylabel =\"Count \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well. I can tell that Texas is often different from the state to which it is compared. But which direction it differs is hard to discern.So maybe it is time to employ some parametric methods after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/11/2023\n",
    "Got all of the state names sorted and counted now need to make the graph look nicer. because it looks very cluttered at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the scatterplot function to build the bubble map\n",
    "sns.scatterplot(data=dataframe_for_rows, x=\"States\", y=\"Count\", size=\"Count\", legend=False, sizes=(20, 200),alpha=0.5)\n",
    "\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/12/2023\n",
    "\n",
    "Going to use the levene test to see if samples come from equal variance distribution\n",
    "\n",
    "If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Levene%27s_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2020[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t#else:\n",
    "\t\t\t\t#continue\n",
    "\t\t#print(stats.levene(lis1, lis2),i)\n",
    "\t\tx.append([subsetb,subsetc,stats.levene(lis1, lis2),i])\n",
    "\t\t#print_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\tresult1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df4['result_string'] = df4[0].astype(str) # gotta change the kurkwalis to string\n",
    "df4['statistic'] = df4['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df4['p-value'] = df4['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df4['State'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df4['state'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ''.join(x[0]))\n",
    "df4[['State1','State2']]=df4['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df4= df4.drop(['result_string',0],axis=1)\n",
    "df4['p-value'] = df4['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df4['p-value'] = pd.to_numeric(df4['p-value'], errors='coerce') # Convert P value to numeric\n",
    "#filtered_df = df4[df4['p-value'] < 0.05]\n",
    "#filtered_df # are there yer to year changes. This code tells us the counts\n",
    "#filtered_df1 = df4[df4['p-value'] > 0.05]\n",
    "#filtered_df1 # are there yer to year changes. This code tells us the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Measure1'] = df4[1].astype(str)\n",
    "df4['Measure1'] = df4['Measure1'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "df4[\"Measure1\"] = df4[\"Measure1\"].str.strip()\n",
    "df4[\"state\"] = df4[\"state\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1['state'] = filtered_df1['state'].str.replace(\"['\", '').str.replace(\"']\", '')\n",
    "filtered_df1[\"state\"] = filtered_df1[\"state\"].str.strip()\n",
    "filtered_df1[\"Measure1\"] = filtered_df1[\"Measure1\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= df4.drop([1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df1= filtered_df1.drop([3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1= filtered_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_move = 'Measure1'\n",
    "\n",
    "# Pop the column and store it in a variable\n",
    "column_temp = df4.pop(column_to_move)\n",
    "\n",
    "# Insert the popped column at the beginning of the DataFrame\n",
    "df4.insert(0, column_to_move, column_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_move = 'state'\n",
    "\n",
    "# Pop the column and store it in a variable\n",
    "column_temp = df4.pop(column_to_move)\n",
    "\n",
    "# Insert the popped column at the beginning of the DataFrame\n",
    "df4.insert(1, column_to_move, column_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_move = 'Measure1'\n",
    "\n",
    "# Pop the column and store it in a variable\n",
    "column_temp = filtered_df_test1.pop(column_to_move)\n",
    "\n",
    "# Insert the popped column at the beginning of the DataFrame\n",
    "filtered_df_test1.insert(0, column_to_move, column_temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1=filtered_df_test1.drop(['Corr'],axis=1)\n",
    "filtered_df_test1=filtered_df_test1.drop(['p-value'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_test1=filtered_df_test1.drop(['State1'],axis=1)\n",
    "filtered_df_test1=filtered_df_test1.drop(['State2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df4[df4.duplicated(['Measure1','state'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df4.drop_duplicates(['Measure1','state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2020_levene = df_no_duplicates.merge(filtered_df_test1, how='right', on=['Measure1','state'])\n",
    "result_2020_levene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So we have the results of the Levene test for 2020 for texas above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T test starts Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(dataframes_list2020)\n",
    "new_list_of_measures=[]\n",
    "print_count = 0\n",
    "\n",
    "for j in range(n):\n",
    "\tmatches_2020 = dataframes_list2020[j] # matches 2020 is first dataframe\n",
    "\tsubseta=(matches_2020['StateAbbr'].unique()) # Just a list of the state names in that df\n",
    "\n",
    "\tif subseta ==['TX']:\n",
    "\t\tmatches_texas = dataframes_list2020[j]  # If subset is correct then just set the variable to be the correct thing\n",
    "\t\tsubsetb = (matches_texas['StateAbbr'].unique())\n",
    "\telse:\n",
    "\t\tnot_texas = dataframes_list2020[j] # Otherwise iterate over everything\n",
    "\t\tsubsetc = (not_texas['StateAbbr'].unique())\n",
    "\t\tsubset1=(matches_texas['Measure'].unique()) \n",
    "\t\tsubset2=(not_texas['Measure'].unique())\n",
    "\t\tintersection_of_measures = set(subset1).intersection(subset2) # now the intersection of all 2\t\n",
    "\t\tintersection_list = list(intersection_of_measures) \n",
    "\tfor i in intersection_list:\n",
    "\t\ty.append([i])\n",
    "\t\tlis1 = matches_texas.loc[matches_texas['Measure']==i,'Data_Value'] # select all of the values in the dataframe that match\n",
    "\t\tlis2 = not_texas.loc[not_texas['Measure']==i,'Data_Value']\n",
    "\t\t#if len(lis1) != len(lis2):\n",
    "\t\t\t#min_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\t#print(min_len)\n",
    "\t\t\t#if len(lis1) == min_len:\n",
    "\t\t\n",
    "\t\t\t\t#lis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\t#print(lis2)\n",
    "\t\t\t#if len(lis2) == min_len:\n",
    "\t\t\t\t#lis1= random.sample(list(lis1), min_len)\n",
    "\t\t\t#else:\n",
    "\t\t\t\t#continue\n",
    "\t\t#print(stats.ttest_ind(lis1, lis2),i)\n",
    "\t\tx.append([subsetb,subsetc,stats.ttest_ind(lis1, lis2),i])\n",
    "\t\tprint_count += 1\n",
    "\t\t#print(\"Total prints:\", print_count)\n",
    "\t\tresult1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.DataFrame(result1)\n",
    "df4['result_string'] = df4[0].astype(str) # gotta change the kurkwalis to string\n",
    "df4['statistic'] = df4['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Then just regex it away\n",
    "df4['p-value'] = df4['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df4['State'] = df4[0].astype(str).str.extractall(r'\\'([A-Z]{2})\\'').groupby(level=0).apply(lambda x: ', '.join(x[0]))\n",
    "df4[['State1','State2']]=df4['State'].apply(lambda x: pd.Series(str(x).split(',')))\n",
    "df4= df4.drop(['result_string',0],axis=1)\n",
    "df4['p-value'] = df4['p-value'].astype(float).apply(lambda x: '{:.10f}'.format(x) if pd.notna(x) else '')\n",
    "df4['p-value'] = pd.to_numeric(df4['p-value'], errors='coerce') # Convert P value to numeric\n",
    "#filtered_df = df4[df4['p-value'] < 0.05]\n",
    "#filtered_df # are there yer to year changes. This code tells us the counts\n",
    "#filtered_df1 = df4[df4['p-value'] > 0.05]\n",
    "#filtered_df1 # are there yer to year changes. This code tells us the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Moday work on comparing all of the T tests for everything  (11/13/2023)\n",
    "# Started on it 11/14/2023\n",
    "To do *If texas and state in lis 1 and lis 2 match AND the measure for filtered_df1. Then Run the equal variance of the scipy.stats.t test\n",
    "Otherwise run unequal variances for the T test\n",
    "\n",
    "\n",
    "* Idea Maybe count all the times all of the states were 'significant' and then put that on a graph by state \n",
    "\n",
    "\n",
    "* Filtering data by measures and then using that to run either equal variance or unequal t test\n",
    "\n",
    "* And no matter what make sure to do 'greater' because you want to compare mean 1 and mean 2. With mean1 being the state in question. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_2020_levene[\"State1\"] = result_2020_levene[\"State1\"].str.strip()\n",
    "result_2020_levene[\"State2\"] = result_2020_levene[\"State2\"].str.strip()\n",
    "result_2020_levene[\"State2\"] = result_2020_levene[\"State2\"].str.strip()\n",
    "result_2020_levene['Measure1'] = result_2020_levene[\"Measure1\"].str.strip()\n",
    "result_2020_levene['Measure'] = result_2020_levene[\"Measure1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = result_2020_levene[result_2020_levene['p-value'] < 0.05] # For levenes test lower p values mean unequal variance\n",
    "defg=result_2020_levene[result_2020_levene['p-value'] >0.05 ] #Equal variance\n",
    "\n",
    "#Levene_test_2020 =defg.groupby('state')['Measure1'].nunique()#\n",
    "#unequal_variances = df2[df2.index.isin(defg.index)]#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if we need this below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "condition1 = s2020.loc[s2020['StateAbbr']==\"TX\"]\n",
    "lis1 = condition1.loc[condition1['Measure']=='Physical_health_not_good_for_>=14_days_among_adults_aged_>=18_years','Data_Value'] # For a dataframe .loc seems to be the simplest for performing this work\n",
    "condition2 = s2020.loc[s2020['StateAbbr']=='NC']\n",
    "lis2 = condition2.loc[condition2['Measure']=='Physical_health_not_good_for_>=14_days_among_adults_aged_>=18_years','Data_Value']\n",
    "\n",
    "if len(lis1) != len(lis2):\n",
    "\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\tprint(min_len)\n",
    "\tif len(lis1) == min_len:\n",
    "\n",
    "\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\tprint(lis2)\n",
    "\tif len(lis2) == min_len:\n",
    "\t\tlis1= random.sample(list(lis1), min_len)\n",
    "print(stats.ttest_ind(lis1, lis2,equal_var=False),i)\n",
    "x.append([subsetb,subsetc,stats.ttest_ind(lis1, lis2),i])\n",
    "print_count += 1\n",
    "print(\"Total prints:\", print_count)\n",
    "result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not sure iif we need the code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below code works at least for now as of 11/14/2023\n",
    "It runs the T test on the full dataframe, matching to the reduced dataframe. And Runs the T test. Printing the T test statistic, appending everything to a list. \n",
    "\n",
    "* If the calculated p-value is below the threshold chosen for statistical significance (usually the 0.10, the 0.05, or 0.01 level), then the null hypothesis is rejected in favor of the alternative hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T test for equal variances for 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get dataframe values in a list\n",
    "prints=defg[['Measure','State1','State2']]\n",
    "n = len(prints)\n",
    "current = prints.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2020['Measure'] = s2020['Measure'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(prints)\n",
    "for i in range(n):  # For i in range n, set the current list parts to separate variables.\n",
    "\tcurrent1 = current[i]\n",
    "\tmeasure=(current1[0])\n",
    "\n",
    "\tstate1=(current1[1]) # first second, third element\n",
    "\tstate2=(current1[2].strip()) # Need to strip random whitesapce\n",
    "\n",
    "\n",
    "\tcondition1 = s2020.loc[s2020['StateAbbr']==state1]\n",
    "\t#print(len(condition1))\n",
    "\tlis1 = condition1.loc[condition1['Measure']==measure,'Data_Value'] # For a dataframe .loc seems to be the simplest for performing this work\n",
    "\tcondition2 = s2020.loc[s2020['StateAbbr']==state2]\n",
    "\t#print(len(condition2))\n",
    "\tlis2 = condition2.loc[condition2['Measure']==measure,'Data_Value']\n",
    "\ty.append([state1,state2])\n",
    "\tif len(lis1) != len(lis2):\n",
    "\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\n",
    "\t\tif len(lis1) == min_len:\n",
    "\n",
    "\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\n",
    "\t\tif len(lis2) == min_len:\n",
    "\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t#print(len(lis1),len(lis2))\n",
    "\t#print(stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i)\n",
    "\tx.append([state1,state2,measure,stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i]) # Make sure to set the alternative to greater\n",
    "\tprint_count += 1\n",
    "\t#print(\"Total prints:\", print_count)\n",
    "\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['result_string'] = df[3].astype(str)\n",
    "df['t-statistic'] = df['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Extract out all of the relevant p values and t statistics\n",
    "df['p-value'] = df['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df['State1']= df[0]\n",
    "df['state2']=df[1]\n",
    "df= df.drop(['result_string',0,1,3,4],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)\n",
    "df_equal_var=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_equal_var_p_gr = df[df['p-value'] > 0.05] # So where p value is greater\n",
    "t_test_equal_var_p_ls = df[df['p-value'] <= 0.05] # So where p value is less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/14/2023 t-test for equal variances for 2020 stops here\n",
    "*Null hypothesis is that the means are the same. And alternative is that they differ. so if p value is low, you reject the null hypothesis. Since p value is not significant, the filtered filtered_df_p_values in the above dataframe are those which there mean for each test value is not higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023\n",
    "T test for unequal variances now for 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get dataframe values in a list\n",
    "prints=abcd[['Measure','State1','State2']]\n",
    "n = len(prints)\n",
    "current = prints.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "n = len(prints)\n",
    "for i in range(n):  # For i in range n, set the current list parts to separate variables.\n",
    "\tcurrent1 = current[i]\n",
    "\tmeasure=(current1[0])\n",
    "\n",
    "\tstate1=(current1[1]) # first second, third element\n",
    "\tstate2=(current1[2].strip()) # Need to strip random whitesapce\n",
    "\n",
    "\n",
    "\tcondition1 = s2020.loc[s2020['StateAbbr']==state1]\n",
    "\t#print(len(condition1))\n",
    "\tlis1 = condition1.loc[condition1['Measure']==measure,'Data_Value'] # For a dataframe .loc seems to be the simplest for performing this work\n",
    "\tcondition2 = s2020.loc[s2020['StateAbbr']==state2]\n",
    "\t#print(len(condition2))\n",
    "\tlis2 = condition2.loc[condition2['Measure']==measure,'Data_Value']\n",
    "\ty.append([state1,state2])\n",
    "\tif len(lis1) != len(lis2):\n",
    "\t\tmin_len = (min(len(lis1), len(lis2)))\n",
    "\t\t\n",
    "\t\tif len(lis1) == min_len:\n",
    "\n",
    "\t\t\tlis2= random.sample(list(lis2), min_len) # Random sample does not use replacement\n",
    "\t\t\t\n",
    "\t\tif len(lis2) == min_len:\n",
    "\t\t\tlis1= random.sample(list(lis1), min_len)\n",
    "\t\t#print(len(lis1),len(lis2))\n",
    "\t#print(stats.ttest_ind(lis1, lis2,equal_var=True,alternative='greater'),i)\n",
    "\tx.append([state1,state2,measure,stats.ttest_ind(lis1, lis2,equal_var=False,alternative='greater'),i]) # Make sure to set the alternative to greater\n",
    "\tprint_count += 1\n",
    "\t#print(\"Total prints:\", print_count)\n",
    "\t#result1 = list(list(t) for t in zip(x, y)) # now just zip all of the lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(x)\n",
    "df['result_string'] = df[3].astype(str)\n",
    "df['t-statistic'] = df['result_string'].str.extract(r'statistic=([0-9.-]+)').astype(float) # Extract out all of the relevant p values and t statistics\n",
    "df['p-value'] = df['result_string'].str.extract(r'pvalue=([0-9.eE+-]+)').astype(float)\n",
    "df['State1']= df[0]\n",
    "df['state2']=df[1]\n",
    "df= df.drop(['result_string',0,1,3,4],axis=1) \n",
    "t_test_unequal_var_p = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_p_gr = df[df['p-value'] > 0.05] # So where p value is greater\n",
    "t_test_unequal_var_p_ls = df[df['p-value'] <= 0.05] # So where p value is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_p_gr.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_p_ls.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023 Need to Go back and fix this. \n",
    "Where means are not different. \n",
    "\n",
    "To visuaklize this maybe.. group the counts? And put it in a bar chart like the one above so the format is the similiar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].nunique()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"Spectral\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/14/2023\n",
    "For t-values data visualizations maybe count where p value is higher for say texas. Then count where it is not, and maybe display by state? \n",
    "\n",
    "Okay so maybe if texas p value is higher (so a low p value) add some number to texas\n",
    "Otherwise add the number to the other state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023\n",
    "The below code needs to compare what is in result list to what is in current1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal Var visualizations For Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints=df_equal_var['state2']  # get the state all by itself\n",
    "prints=prints.unique() \n",
    "lists1=[]\n",
    "for i in prints:   \n",
    "\tabc = df_equal_var.loc[df_equal_var['state2']==i]\n",
    "\tabcd = abc[abc['p-value'] <= 0.05]\n",
    "\tdefg = abc[abc['p-value'] > 0.05]\n",
    "\tlists1.append(['TX',len(abcd),i,len(defg)])\n",
    "\tset_of_tuples = {tuple(inner_list) for inner_list in lists1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame(set_of_tuples)\n",
    "df10['combined'] = df10[0]+'/'+df10[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = range(len(set_of_tuples))\n",
    "x_val= [x[0] for x in set_of_tuples]\n",
    "y_val = [x[1] for x in set_of_tuples]\n",
    "x_val1 = [x[2] for x in set_of_tuples]\n",
    "y_val1 =[x[3] for x in set_of_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(data=(y_val),alpha=0.9)\n",
    "ax = sns.barplot(data=y_val1, alpha=0.7)\n",
    "ax.set_xticklabels(df10['combined'], rotation='vertical', fontsize=10)\n",
    "ax.set(xlabel='States',\n",
    "       ylabel='count of p_values that are significant per state VS Texas',\n",
    "       title='Resulting P values'\n",
    "       )\n",
    "ax.text(2, 9, \"Texas = ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"blue\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(2.8, 10,\"Other States= ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"orange\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(2.8, 11,\"Texas Overlap = ♦\", \n",
    "       fontsize = 10,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"brown\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/14/2023 USE THIS\n",
    "Simpler Is better. So just loc the result dataframe and do the lengths of each. \n",
    "The code right below takes the length of the state names list and does.. something. \\\n",
    "I guess sorts and counts and so I have to append those counts to some kind of list. \n",
    "\n",
    "But now I can visualize the data properly. By taking everything out of the set of tuples and making a bar graph like the one for the original data\n",
    "\n",
    "## TO DO 11/15/2023 The below is for unequal var\n",
    "visualize the p values for texas in front of (or behind) every other state\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_unequal_var_p[t_test_unequal_var_p['p-value'] > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints=t_test_unequal_var_p['state2']  # get the state all by itself\n",
    "prints=prints.unique() \n",
    "lists1=[]\n",
    "for i in prints:   \n",
    "\tabc = df.loc[df['state2']==i]\n",
    "\tabcd = abc[abc['p-value'] < 0.05]\n",
    "\tdefg = abc[abc['p-value'] > 0.05]\n",
    "\tlists1.append(['TX',len(abcd),i,len(defg)])\n",
    "\tset_of_tuples = {tuple(inner_list) for inner_list in lists1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.DataFrame(set_of_tuples)\n",
    "df10['combined'] = df10[0]+'/'+df10[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = range(len(set_of_tuples))\n",
    "x_val= [x[0] for x in set_of_tuples]\n",
    "y_val = [x[1] for x in set_of_tuples]\n",
    "x_val1 = [x[2] for x in set_of_tuples]\n",
    "y_val1 =[x[3] for x in set_of_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(data=(y_val),alpha=0.9)\n",
    "ax = sns.barplot(data=y_val1, alpha=0.7)\n",
    "ax.set_xticklabels(df10['combined'], rotation='vertical', fontsize=10)\n",
    "ax.set(xlabel='States',\n",
    "       ylabel='count of p_values that are significant per state VS Texas',\n",
    "       title='Resulting P values'\n",
    "       )\n",
    "ax.text(2.2, 20, \" Texas = ♦\", \n",
    "       fontsize = 12,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"blue\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(4, 19,\"All other States = ♦\", \n",
    "       fontsize = 12,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"orange\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "ax.text(3.8, 18,\"Texas Overlap = ♦\", \n",
    "       fontsize = 12,          # Size\n",
    "       fontstyle = \"oblique\",  # Style\n",
    "       color = \"brown\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at the dataframe above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/15/2023 \n",
    "Looking over the data, when compared about half of the values have significane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code right above is the list organization code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts1 = s2020.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts1 = city_counts1.sort_values(ascending=False)\n",
    "city_counts2 = s2021.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts2 = city_counts2.sort_values(ascending=False)\n",
    "city_counts3 = s2022.groupby('StateAbbr')['LocationName'].count()\n",
    "city_counts3 = city_counts3.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.barplot(city_counts1,palette=\"Greens\")\n",
    "\n",
    "ax.set(xlabel=\"State\",ylabel =\"Number of locations \",title=\"Locations per State 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above is all well and good. (11/10/2023)\n",
    "But it still does not answer my question. Just states that they are different. \n",
    "So I am going to attempt a T test on Monday. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working down here for correlations as of 11/15/2023\n",
    "\n",
    "Trying to organize my values and decide if the values should be dropped. The below code filtered out all of the combinations that had corr values that were not significant. As far as how to move forward. Not sure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
